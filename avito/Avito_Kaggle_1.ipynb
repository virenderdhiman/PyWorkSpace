{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\dhiman\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\dhiman\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This model is designed to use embeddings to deal with the categorical variables, while ignoring \n",
    "some of the data like the periods, images, titles and descriptions. \n",
    "Most of the remaining data is categorical, except for the price. \n",
    "A simple FFNN model seems to do pretty good work. \n",
    "\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input, Embedding, Dropout, Flatten\n",
    "from keras.layers.merge import concatenate, dot, multiply, add\n",
    "from keras.callbacks import Callback, EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras.optimizers import Nadam, RMSprop, adam\n",
    "from keras.layers.noise import AlphaDropout, GaussianNoise\n",
    "from keras import backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import re\n",
    "import pickle \n",
    "#import mglearn\n",
    "import time\n",
    "import string\n",
    "from nltk.tokenize import TweetTokenizer # doesn't split at apostrophes\n",
    "import nltk\n",
    "from nltk import Text\n",
    "from nltk.tokenize import regexp_tokenize\n",
    "from nltk.tokenize import word_tokenize  \n",
    "from nltk.tokenize import sent_tokenize \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "seed=45\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "##================Import Data, Replace NaNs with -1\n",
    "data_dir = \"input/\"\n",
    "train_data = pd.read_csv(data_dir+\"/train.csv\", parse_dates=[\"activation_date\"]) #we will eventually turn the date column into day of week [0,6]\n",
    "test_data  = pd.read_csv(data_dir+\"/test.csv\", parse_dates=[\"activation_date\"])\n",
    "train_data = train_data.replace(np.nan,-1,regex=True) #nan and other missing values are mapped to -1\n",
    "test_data  = test_data.replace(np.nan,-1,regex=True)\n",
    "\n",
    "##================Remove unwanted columns\n",
    "del train_data['image'], test_data['image'],train_data['user_id'],\n",
    "test_data['user_id'],train_data['item_id'],test_data['item_id']\n",
    "\n",
    "##================Replace Full Dates with Day-of-Week\n",
    "train_data['activation_date'] = train_data[\"activation_date\"].dt.weekday\n",
    "test_data['activation_date'] = test_data[\"activation_date\"].dt.weekday\n",
    "\n",
    "##================split into x_train/x_val. No stratification requried probably\n",
    "val_split = 0.15\n",
    "train_data = train_data.sample(frac=1,random_state=seed).reset_index(drop=True)\n",
    "val_ix = int(np.rint(len(train_data)*(1.-val_split)))\n",
    "#data frame formats with y-values packed in\n",
    "train_df = train_data[:val_ix]\n",
    "val_df = train_data[val_ix:]\n",
    "test_df = test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'МКМ длина 4.5м ширина 1.6м высота борта 0.6м/\\nПассажировместимость 4 чел /\\nГрузоподъемность 400 кг/\\nЗав номер СВ 2К 1131/\\nТорг'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text=train_df['description'][24]\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['мкм', 'длин', 'ширин', 'высот', 'борт', 'пассажировместим', 'чел', 'грузоподъемн', 'кг', 'зав', 'номер', 'св', 'торг']\n"
     ]
    }
   ],
   "source": [
    "#Working on Russian text\n",
    "# split into words\n",
    "tokens = word_tokenize(text)\n",
    "# convert to lower case\n",
    "table = str.maketrans('', '', string.punctuation)\n",
    "tokens = [SnowballStemmer(\"russian\").stem(w.lower().translate(table))\n",
    "          for w in tokens if w.lower().translate(table).isalpha()]\n",
    "# remove punctuation from each word\n",
    "\n",
    "\n",
    "#stripped = [w.translate(table) for w in tokens]\n",
    "# remove remaining tokens that are not alphabetic\n",
    "#words = [word for word in stripped if word.isalpha()]\n",
    "# filter out stop words\n",
    "#stop_words = set(stopwords.words('russian'))\n",
    "#words = [w for w in words if not w in stop_words]\n",
    "#words_stemmed = [SnowballStemmer(\"russian\").stem(w) for w in words if not w in stop_words]\n",
    "#stemmer = SnowballStemmer(\"russian\")\n",
    "#words_stemmed=[stemmer.stem(word) for word in words]\n",
    "#print(words)\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_tokenizer(text):   \n",
    "    tokens = word_tokenize(text)    \n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    tokens_stem = [SnowballStemmer(\"russian\").stem(w.lower().translate(table))\n",
    "                   for w in tokens if w.lower().translate(table).isalpha()]\n",
    "    return tokens_stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-a0d9f070f033>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m#title_te = tf.fit_transform(train_df['title'])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mtitle_fitted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'title'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mtitle_transformed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtitle_fitted\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'title'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mdesc_fitted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'description'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mdesc_transformed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtitle_fitted\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'description'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mtransform\u001b[1;34m(self, raw_documents, copy)\u001b[0m\n\u001b[0;32m   1407\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'_tfidf'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'The tfidf vector is not fitted'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1408\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1409\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTfidfVectorizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1410\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mtransform\u001b[1;34m(self, raw_documents)\u001b[0m\n\u001b[0;32m    921\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    922\u001b[0m         \u001b[1;31m# use the same matrix-building strategy as fit_transform\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 923\u001b[1;33m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_count_vocab\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfixed_vocab\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    924\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    925\u001b[0m             \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfill\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m    790\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    791\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 792\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[1;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    793\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    794\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(doc)\u001b[0m\n\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    265\u001b[0m             return lambda doc: self._word_ngrams(\n\u001b[1;32m--> 266\u001b[1;33m                 tokenize(preprocess(self.decode(doc))), stop_words)\n\u001b[0m\u001b[0;32m    267\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    268\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-25-a33a8ebc65e5>\u001b[0m in \u001b[0;36mword_tokenizer\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mword_tokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mtokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mtable\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmaketrans\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpunctuation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     tokens_stem = [SnowballStemmer(\"russian\").stem(w.lower().translate(table))\n\u001b[0;32m      5\u001b[0m                    for w in tokens if w.lower().translate(table).isalpha()]\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\nltk\\tokenize\\__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[1;34m(text, language, preserve_line)\u001b[0m\n\u001b[0;32m    127\u001b[0m     \"\"\"\n\u001b[0;32m    128\u001b[0m     \u001b[0msentences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 129\u001b[1;33m     return [token for sent in sentences\n\u001b[0m\u001b[0;32m    130\u001b[0m             for token in _treebank_word_tokenizer.tokenize(sent)]\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\nltk\\tokenize\\__init__.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    128\u001b[0m     \u001b[0msentences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m     return [token for sent in sentences\n\u001b[1;32m--> 130\u001b[1;33m             for token in _treebank_word_tokenizer.tokenize(sent)]\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\nltk\\tokenize\\treebank.py\u001b[0m in \u001b[0;36mtokenize\u001b[1;34m(self, text, convert_parentheses, return_str)\u001b[0m\n\u001b[0;32m    130\u001b[0m         \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\" \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mtext\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\" \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    131\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 132\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mregexp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msubstitution\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mENDING_QUOTES\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    133\u001b[0m             \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mregexp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubstitution\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tf = TfidfVectorizer(smooth_idf=False, sublinear_tf=False, norm=None, analyzer='word',\n",
    "                     stop_words=stopwords.words('russian'), encoding='KOI8-R' , tokenizer=word_tokenizer)\n",
    "#title_te = tf.fit_transform(train_df['title']) \n",
    "title_fitted = tf.fit(train_df['title'])\n",
    "title_transformed = title_fitted.transform(train_df['title'])\n",
    "desc_fitted = tf.fit(train_df['description'])\n",
    "desc_transformed = title_fitted.transform(train_df['description'])\n",
    "\n",
    "#desc_te = tf.fit_transform(train_df['description']) \n",
    "#print (\"The text: \", txt1)\n",
    "#print (\"The txt_transformed: \", txt_transformed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The text:  TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='KOI8-R', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), norm=None, preprocessor=None, smooth_idf=False,\n",
      "        stop_words=['и', 'в', 'во', 'не', 'что', 'он', 'на', 'я', 'с', 'со', 'как', 'а', 'то', 'все', 'она', 'так', 'его', 'но', 'да', 'ты', 'к', 'у', 'же', 'вы', 'за', 'бы', 'по', 'только', 'ее', 'мне', 'было', 'вот', 'от', 'меня', 'еще', 'нет', 'о', 'из', 'ему', 'теперь', 'когда', 'даже', 'ну', 'вдруг', '...гда', 'лучше', 'чуть', 'том', 'нельзя', 'такой', 'им', 'более', 'всегда', 'конечно', 'всю', 'между'],\n",
      "        strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None)\n"
     ]
    }
   ],
   "source": [
    "print (\"The text: \", txt_fitted)\n",
    "#print (\"The txt_transformed: \", txt_transformed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1277910, 188294)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt_transformed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-a53bd52b002c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'title_te'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'title'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mget_Stemmed_words\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[0;32m   2549\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2550\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masobject\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2551\u001b[1;33m                 \u001b[0mmapped\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2552\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2553\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/src/inference.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m<ipython-input-30-a53bd52b002c>\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'title_te'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'title'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mget_Stemmed_words\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-24-a76668411cb0>\u001b[0m in \u001b[0;36mget_Stemmed_words\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mwords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mstripped\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misalpha\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[1;31m# filter out stop words\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m     \u001b[0mstop_words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstopwords\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'russian'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m     \u001b[1;31m#words = [w for w in words if not w in stop_words]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[0mwords_stemmed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mSnowballStemmer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"russian\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mwords\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mstop_words\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\nltk\\corpus\\reader\\wordlist.py\u001b[0m in \u001b[0;36mwords\u001b[1;34m(self, fileids, ignore_lines_startswith)\u001b[0m\n\u001b[0;32m     20\u001b[0m     \"\"\"\n\u001b[0;32m     21\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mwords\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfileids\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_lines_startswith\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m         return [line for line in line_tokenize(self.raw(fileids))\n\u001b[0m\u001b[0;32m     23\u001b[0m                 if not line.startswith(ignore_lines_startswith)]\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\nltk\\corpus\\reader\\wordlist.py\u001b[0m in \u001b[0;36mraw\u001b[1;34m(self, fileids)\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mfileids\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfileids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fileids\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfileids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfileids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfileids\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfileids\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\nltk\\corpus\\reader\\wordlist.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mfileids\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfileids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fileids\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfileids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfileids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfileids\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfileids\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\nltk\\corpus\\reader\\api.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(self, file)\u001b[0m\n\u001b[0;32m    211\u001b[0m         \"\"\"\n\u001b[0;32m    212\u001b[0m         \u001b[0mencoding\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 213\u001b[1;33m         \u001b[0mstream\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_root\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    214\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mstream\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mjoin\u001b[1;34m(self, fileid)\u001b[0m\n\u001b[0;32m    336\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfileid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    337\u001b[0m         \u001b[0m_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfileid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 338\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mFileSystemPathPointer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    339\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    340\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\nltk\\compat.py\u001b[0m in \u001b[0;36m_decorator\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    219\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_decorator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    220\u001b[0m         \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madd_py3_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 221\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0minit_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    222\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mwraps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minit_func\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_decorator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, _path)\u001b[0m\n\u001b[0;32m    313\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    314\u001b[0m         \u001b[0m_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mabspath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 315\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    316\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'No such file or directory: %r'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0m_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    317\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_path\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\genericpath.py\u001b[0m in \u001b[0;36mexists\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[1;34m\"\"\"Test whether a path exists.  Returns False for broken symbolic links\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m         \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_df['title_te']=train_df['title'].apply(lambda x: get_Stemmed_words(str(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-108-4664a7c5003a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'description_te'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'description'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[0;32m   2549\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2550\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masobject\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2551\u001b[1;33m                 \u001b[0mmapped\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2552\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2553\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/src/inference.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m<ipython-input-108-4664a7c5003a>\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'description_te'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'description'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\nltk\\tokenize\\__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[1;34m(text, language, preserve_line)\u001b[0m\n\u001b[0;32m    127\u001b[0m     \"\"\"\n\u001b[0;32m    128\u001b[0m     \u001b[0msentences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 129\u001b[1;33m     return [token for sent in sentences\n\u001b[0m\u001b[0;32m    130\u001b[0m             for token in _treebank_word_tokenizer.tokenize(sent)]\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\nltk\\tokenize\\__init__.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    128\u001b[0m     \u001b[0msentences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m     return [token for sent in sentences\n\u001b[1;32m--> 130\u001b[1;33m             for token in _treebank_word_tokenizer.tokenize(sent)]\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\nltk\\tokenize\\treebank.py\u001b[0m in \u001b[0;36mtokenize\u001b[1;34m(self, text, convert_parentheses, return_str)\u001b[0m\n\u001b[0;32m    136\u001b[0m             \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mregexp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr' \\1 \\2 '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mregexp\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCONTRACTIONS3\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 138\u001b[1;33m             \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mregexp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr' \\1 \\2 '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    139\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m         \u001b[1;31m# We are not using CONTRACTIONS4 since\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_df['description_te']=train_df['description'].apply(lambda x: get_Stemmed_words(str(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Script by https://www.kaggle.com/ogrellier\n",
    "# Code: https://www.kaggle.com/ogrellier/python-target-encoding-for-categorical-features\n",
    "def add_noise(series, noise_level):\n",
    "    return series * (1 + noise_level * np.random.randn(len(series)))\n",
    "\n",
    "def target_encode_train(trn_series=None,\n",
    "                  target=None, \n",
    "                  min_samples_leaf=1, \n",
    "                  smoothing=1,\n",
    "                  noise_level=0):\n",
    "    \"\"\"\n",
    "    Smoothing is computed like in the following paper by Daniele Micci-Barreca\n",
    "    https://kaggle2.blob.core.windows.net/forum-message-attachments/225952/7441/high%20cardinality%20categoricals.pdf\n",
    "    trn_series : training categorical feature as a pd.Series\n",
    "    tst_series : test categorical feature as a pd.Series\n",
    "    target : target data as a pd.Series\n",
    "    min_samples_leaf (int) : minimum samples to take category average into account\n",
    "    smoothing (int) : smoothing effect to balance categorical average vs prior  \n",
    "    \"\"\" \n",
    "    assert len(trn_series) == len(target)\n",
    "    temp = pd.concat([trn_series, target], axis=1)\n",
    "    # Compute target mean \n",
    "    averages = temp.groupby(by=trn_series.name)[target.name].agg([\"mean\", \"count\"])\n",
    "    # Compute smoothing\n",
    "    smoothing = 1 / (1 + np.exp(-(averages[\"count\"] - min_samples_leaf) / smoothing))\n",
    "    # Apply average function to all target data\n",
    "    prior = target.mean()\n",
    "    # The bigger the count the less full_avg is taken into account\n",
    "    averages[target.name] = prior * (1 - smoothing) + averages[\"mean\"] * smoothing\n",
    "    averages.drop([\"mean\", \"count\"], axis=1, inplace=True)\n",
    "    # Apply averages to trn and tst series\n",
    "    ft_trn_series = pd.merge(\n",
    "        trn_series.to_frame(trn_series.name),\n",
    "        averages.reset_index().rename(columns={'index': target.name, target.name: 'average'}),\n",
    "        on=trn_series.name,\n",
    "        how='left')['average'].rename(trn_series.name + '_mean').fillna(prior)\n",
    "    # pd.merge does not keep the index so restore it\n",
    "    ft_trn_series.index = trn_series.index \n",
    "    \n",
    "    return add_noise(ft_trn_series, noise_level)\n",
    "\n",
    "def target_encode_test(tst_series=None, \n",
    "                  target=None, \n",
    "                  min_samples_leaf=1, \n",
    "                  smoothing=1,\n",
    "                  noise_level=0):\n",
    "    \"\"\"\n",
    "    Smoothing is computed like in the following paper by Daniele Micci-Barreca\n",
    "    https://kaggle2.blob.core.windows.net/forum-message-attachments/225952/7441/high%20cardinality%20categoricals.pdf\n",
    "    trn_series : training categorical feature as a pd.Series\n",
    "    tst_series : test categorical feature as a pd.Series\n",
    "    target : target data as a pd.Series\n",
    "    min_samples_leaf (int) : minimum samples to take category average into account\n",
    "    smoothing (int) : smoothing effect to balance categorical average vs prior  \n",
    "    \"\"\" \n",
    "    prior = target.mean()\n",
    "    ft_tst_series = pd.merge(\n",
    "        tst_series.to_frame(tst_series.name),\n",
    "        averages.reset_index().rename(columns={'index': target.name, target.name: 'average'}),\n",
    "        on=tst_series.name,\n",
    "        how='left')['average'].rename(trn_series.name + '_mean').fillna(prior)\n",
    "    # pd.merge does not keep the index so restore it\n",
    "    ft_tst_series.index = tst_series.index\n",
    "    return add_noise(ft_trn_series, noise_level), add_noise(ft_tst_series, noise_level)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item_id</th>\n",
       "      <th>region</th>\n",
       "      <th>city</th>\n",
       "      <th>parent_category_name</th>\n",
       "      <th>category_name</th>\n",
       "      <th>param_1</th>\n",
       "      <th>param_2</th>\n",
       "      <th>param_3</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>price</th>\n",
       "      <th>item_seq_number</th>\n",
       "      <th>activation_date</th>\n",
       "      <th>user_type</th>\n",
       "      <th>image_top_1</th>\n",
       "      <th>deal_probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9076ab760743</td>\n",
       "      <td>Ярославская область</td>\n",
       "      <td>Ярославль</td>\n",
       "      <td>Для дома и дачи</td>\n",
       "      <td>Мебель и интерьер</td>\n",
       "      <td>Освещение</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>Продам торшер</td>\n",
       "      <td>торшер ссср</td>\n",
       "      <td>500.0</td>\n",
       "      <td>89</td>\n",
       "      <td>5</td>\n",
       "      <td>Company</td>\n",
       "      <td>1463.0</td>\n",
       "      <td>0.86521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6ac30da289d4</td>\n",
       "      <td>Самарская область</td>\n",
       "      <td>Самара</td>\n",
       "      <td>Хобби и отдых</td>\n",
       "      <td>Коллекционирование</td>\n",
       "      <td>Другое</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>Радио-лампа</td>\n",
       "      <td>новая не б-у</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>201</td>\n",
       "      <td>1</td>\n",
       "      <td>Private</td>\n",
       "      <td>3016.0</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>c053b75738a7</td>\n",
       "      <td>Свердловская область</td>\n",
       "      <td>Екатеринбург</td>\n",
       "      <td>Хобби и отдых</td>\n",
       "      <td>Спорт и отдых</td>\n",
       "      <td>Другое</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>Палочка для художественной гимнастики</td>\n",
       "      <td>Палочка для художественной гимнастики SASAKI, ...</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>Private</td>\n",
       "      <td>2732.0</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>f0e4c86b84fc</td>\n",
       "      <td>Иркутская область</td>\n",
       "      <td>Ангарск</td>\n",
       "      <td>Личные вещи</td>\n",
       "      <td>Детская одежда и обувь</td>\n",
       "      <td>Для мальчиков</td>\n",
       "      <td>Верхняя одежда</td>\n",
       "      <td>110-116 см (4-6 лет)</td>\n",
       "      <td>Весенний костюм</td>\n",
       "      <td>Продам отличный весенний костюм на мальчика. С...</td>\n",
       "      <td>900.0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>Private</td>\n",
       "      <td>51.0</td>\n",
       "      <td>0.13726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>b04e2ae871f3</td>\n",
       "      <td>Новосибирская область</td>\n",
       "      <td>Новосибирск</td>\n",
       "      <td>Бытовая электроника</td>\n",
       "      <td>Телефоны</td>\n",
       "      <td>iPhone</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>iPhone 4 16GB</td>\n",
       "      <td>Продам телефон, лежит уже пол года без дела тк...</td>\n",
       "      <td>2500.0</td>\n",
       "      <td>14</td>\n",
       "      <td>2</td>\n",
       "      <td>Private</td>\n",
       "      <td>2918.0</td>\n",
       "      <td>0.43177</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        item_id                 region          city parent_category_name  \\\n",
       "0  9076ab760743    Ярославская область     Ярославль      Для дома и дачи   \n",
       "1  6ac30da289d4      Самарская область        Самара        Хобби и отдых   \n",
       "2  c053b75738a7   Свердловская область  Екатеринбург        Хобби и отдых   \n",
       "3  f0e4c86b84fc      Иркутская область       Ангарск          Личные вещи   \n",
       "4  b04e2ae871f3  Новосибирская область   Новосибирск  Бытовая электроника   \n",
       "\n",
       "            category_name        param_1         param_2  \\\n",
       "0       Мебель и интерьер      Освещение              -1   \n",
       "1      Коллекционирование         Другое              -1   \n",
       "2           Спорт и отдых         Другое              -1   \n",
       "3  Детская одежда и обувь  Для мальчиков  Верхняя одежда   \n",
       "4                Телефоны         iPhone              -1   \n",
       "\n",
       "                param_3                                  title  \\\n",
       "0                    -1                          Продам торшер   \n",
       "1                    -1                            Радио-лампа   \n",
       "2                    -1  Палочка для художественной гимнастики   \n",
       "3  110-116 см (4-6 лет)                        Весенний костюм   \n",
       "4                    -1                          iPhone 4 16GB   \n",
       "\n",
       "                                         description   price  item_seq_number  \\\n",
       "0                                        торшер ссср   500.0               89   \n",
       "1                                       новая не б-у    -1.0              201   \n",
       "2  Палочка для художественной гимнастики SASAKI, ...  2000.0               15   \n",
       "3  Продам отличный весенний костюм на мальчика. С...   900.0                1   \n",
       "4  Продам телефон, лежит уже пол года без дела тк...  2500.0               14   \n",
       "\n",
       "   activation_date user_type  image_top_1  deal_probability  \n",
       "0                5   Company       1463.0           0.86521  \n",
       "1                1   Private       3016.0           0.00000  \n",
       "2                0   Private       2732.0           0.00000  \n",
       "3                4   Private         51.0           0.13726  \n",
       "4                2   Private       2918.0           0.43177  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dhiman\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "train_encoded = target_encode_train(train_df[\"city\"],\n",
    "                             target='deal_probability', \n",
    "                             min_samples_leaf=100,\n",
    "                             smoothing=10,\n",
    "                             noise_level=0.01)\n",
    "    \n",
    "train_df['city_te'] = train_encoded\n",
    "\n",
    "\n",
    "#test.drop('ps_car_11_cat', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'deal_probability'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-76-c1704e9ba711>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m test_encoded = target_encode_train(test_df[\"city\"],\n\u001b[1;32m----> 2\u001b[1;33m                              \u001b[0mtarget\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtest_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdeal_probability\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m                              \u001b[0mmin_samples_leaf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m                              \u001b[0msmoothing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m                              noise_level=0.01)\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   3612\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3613\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3614\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3615\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3616\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'deal_probability'"
     ]
    }
   ],
   "source": [
    "test_encoded = target_encode_train(test_df[\"city\"],\n",
    "                             target=deal_probability, \n",
    "                             min_samples_leaf=100,\n",
    "                             smoothing=10,\n",
    "                             noise_level=0.01)\n",
    "\n",
    "#train.drop('ps_car_11_cat', axis=1, inplace=True)\n",
    "#meta.loc['ps_car_11_cat','keep'] = False  # Updating the meta\n",
    "test_df['city_te'] = test_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.12794187, 0.13924898, 0.12490264, ..., 0.12489774, 0.1402812 ,\n",
       "       0.13622217])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.asarray(train_df['city_te'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1722"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_df.city.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Ярославль': 1,\n",
       " 'Самара': 2,\n",
       " 'Екатеринбург': 3,\n",
       " 'Ангарск': 4,\n",
       " 'Новосибирск': 5,\n",
       " 'Тюмень': 6,\n",
       " 'Стерлитамак': 7,\n",
       " 'Сочи': 8,\n",
       " 'Краснодар': 9,\n",
       " 'Чапаевск': 10,\n",
       " 'Иркутск': 11,\n",
       " 'Уфа': 12,\n",
       " 'Набережные Челны': 13,\n",
       " 'Ижевск': 14,\n",
       " 'Калининград': 15,\n",
       " 'Саратов': 16,\n",
       " 'Анапа': 17,\n",
       " 'Новороссийск': 18,\n",
       " 'Альметьевск': 19,\n",
       " 'Вольск': 20,\n",
       " 'Нижний Новгород': 21,\n",
       " 'Челябинск': 22,\n",
       " 'Карталы': 23,\n",
       " 'Минусинск': 24,\n",
       " 'Белгород': 25,\n",
       " 'Волгоград': 26,\n",
       " 'Васильково': 27,\n",
       " 'Тула': 28,\n",
       " 'Усть-Илимск': 29,\n",
       " 'Тольятти': 30,\n",
       " 'Пермь': 31,\n",
       " 'Омск': 32,\n",
       " 'Красноярск': 33,\n",
       " 'Качканар': 34,\n",
       " 'Магнитогорск': 35,\n",
       " 'Павлово': 36,\n",
       " 'Казань': 37,\n",
       " 'Ханты-Мансийск': 38,\n",
       " 'Оренбург': 39,\n",
       " 'Братск': 40,\n",
       " 'Новокузнецк': 41,\n",
       " 'Морозовск': 42,\n",
       " 'Воронеж': 43,\n",
       " 'Барнаул': 44,\n",
       " 'Чернушка': 45,\n",
       " 'Тайшет': 46,\n",
       " 'Белокуриха': 47,\n",
       " 'Междуреченск': 48,\n",
       " 'Ставрополь': 49,\n",
       " 'Нижний Тагил': 50,\n",
       " 'Сургут': 51,\n",
       " 'Ростов-на-Дону': 52,\n",
       " 'Миасс': 53,\n",
       " 'Гусь-Хрустальный': 54,\n",
       " 'Будённовск': 55,\n",
       " 'Канск': 56,\n",
       " 'Гурьевск': 57,\n",
       " 'Ачинск': 58,\n",
       " 'Кемерово': 59,\n",
       " 'Нефтекамск': 60,\n",
       " 'Таганрог': 61,\n",
       " 'Бийск': 62,\n",
       " 'Каменск-Уральский': 63,\n",
       " 'Чебаркуль': 64,\n",
       " 'Ейск': 65,\n",
       " 'Зеленодольск': 66,\n",
       " 'Белебей': 67,\n",
       " 'Дзержинск': 68,\n",
       " 'Геленджик': 69,\n",
       " 'Нижневартовск': 70,\n",
       " 'Кропоткин': 71,\n",
       " 'Бердск': 72,\n",
       " 'Дюртюли': 73,\n",
       " 'Краснотурьинск': 74,\n",
       " 'Серов': 75,\n",
       " 'Урюпинск': 76,\n",
       " 'Пятигорск': 77,\n",
       " 'Владимир': 78,\n",
       " 'Соликамск': 79,\n",
       " 'Ревда': 80,\n",
       " 'Тимашевск': 81,\n",
       " 'Новоалтайск': 82,\n",
       " 'Волгодонск': 83,\n",
       " 'Нефтеюганск': 84,\n",
       " 'Лабинск': 85,\n",
       " 'Буинск': 86,\n",
       " 'Пойковский': 87,\n",
       " 'Камышин': 88,\n",
       " 'Псебай': 89,\n",
       " 'Миллерово': 90,\n",
       " 'Сосновоборск': 91,\n",
       " 'Волжский': 92,\n",
       " 'Ессентуки': 93,\n",
       " 'Усть-Катав': 94,\n",
       " 'Троицк': 95,\n",
       " 'Энгельс': 96,\n",
       " 'Курганинск': 97,\n",
       " 'Выкса': 98,\n",
       " 'Бузулук': 99,\n",
       " 'Алексин': 100,\n",
       " 'Кинель': 101,\n",
       " 'Кукмор': 102,\n",
       " 'Белорецк': 103,\n",
       " 'Орск': 104,\n",
       " 'Нягань': 105,\n",
       " 'Новомосковск': 106,\n",
       " 'Сызрань': 107,\n",
       " 'Янаул': 108,\n",
       " 'Варениковская': 109,\n",
       " 'Давлеканово': 110,\n",
       " 'Кисловодск': 111,\n",
       " 'Муром': 112,\n",
       " 'Коркино': 113,\n",
       " 'Норильск': 114,\n",
       " 'Мелеуз': 115,\n",
       " 'Шиханы': 116,\n",
       " 'Чистополь': 117,\n",
       " 'Чайковский': 118,\n",
       " 'Ишим': 119,\n",
       " 'Октябрьский': 120,\n",
       " 'Ковров': 121,\n",
       " 'Учалы': 122,\n",
       " 'Кулебаки': 123,\n",
       " 'Первоуральск': 124,\n",
       " 'Новозаведенное': 125,\n",
       " 'Шарыпово': 126,\n",
       " 'Березники': 127,\n",
       " 'Щекино': 128,\n",
       " 'Кстово': 129,\n",
       " 'Новый Оскол': 130,\n",
       " 'Невинномысск': 131,\n",
       " 'Зайково': 132,\n",
       " 'Кормиловка': 133,\n",
       " 'Борисоглебск': 134,\n",
       " 'Староминская': 135,\n",
       " 'Похвистнево': 136,\n",
       " 'Новоуральск': 137,\n",
       " 'Минеральные Воды': 138,\n",
       " 'Мегион': 139,\n",
       " 'Абинск': 140,\n",
       " 'Игра': 141,\n",
       " 'Бугуруслан': 142,\n",
       " 'Березовский': 143,\n",
       " 'Заринск': 144,\n",
       " 'Кущевская': 145,\n",
       " 'Сарапул': 146,\n",
       " 'Кувандык': 147,\n",
       " 'Ялуторовск': 148,\n",
       " 'Михайловск': 149,\n",
       " 'Георгиевск': 150,\n",
       " 'Анастасиевская': 151,\n",
       " 'Шахты': 152,\n",
       " 'Кумертау': 153,\n",
       " 'Александров': 154,\n",
       " 'Вязники': 155,\n",
       " 'Лысково': 156,\n",
       " 'Россошь': 157,\n",
       " 'Прокопьевск': 158,\n",
       " 'Киржач': 159,\n",
       " 'Верхняя Салда': 160,\n",
       " 'Дружинино': 161,\n",
       " 'Рыбинск': 162,\n",
       " 'Искитим': 163,\n",
       " 'Азов': 164,\n",
       " 'Белово': 165,\n",
       " 'Некрасовская': 166,\n",
       " 'Южноуральск': 167,\n",
       " 'Нижнекамск': 168,\n",
       " 'Константиновск': 169,\n",
       " 'Заводоуковск': 170,\n",
       " 'Упорово': 171,\n",
       " 'Гражданское': 172,\n",
       " 'Гулькевичи': 173,\n",
       " 'Армавир': 174,\n",
       " 'Тобольск': 175,\n",
       " 'Бачатский': 176,\n",
       " 'Советск': 177,\n",
       " 'Каменка': 178,\n",
       " 'Глазов': 179,\n",
       " 'Арзамас': 180,\n",
       " 'Усолье-Сибирское': 181,\n",
       " 'Ипатово': 182,\n",
       " 'Славянск-на-Кубани': 183,\n",
       " 'Полевской': 184,\n",
       " 'Балаково': 185,\n",
       " 'Тара': 186,\n",
       " 'Хадыженск': 187,\n",
       " 'Тихорецк': 188,\n",
       " 'Темрюк': 189,\n",
       " 'Карпинск': 190,\n",
       " 'Клетская': 191,\n",
       " 'Отрадный': 192,\n",
       " 'Шебекино': 193,\n",
       " 'Копейск': 194,\n",
       " 'Узловая': 195,\n",
       " 'Горячий Ключ': 196,\n",
       " 'Юрга': 197,\n",
       " 'Павловская': 198,\n",
       " 'Нурлат': 199,\n",
       " 'Рубцовск': 200,\n",
       " 'Афипский': 201,\n",
       " 'Салават': 202,\n",
       " 'Прибельский': 203,\n",
       " 'Москаленки': 204,\n",
       " 'Арамиль': 205,\n",
       " 'Строитель': 206,\n",
       " 'Навашино': 207,\n",
       " 'Переславль-Залесский': 208,\n",
       " 'Мамадыш': 209,\n",
       " 'Верхняя Пышма': 210,\n",
       " 'Чернянка': 211,\n",
       " 'Асбест': 212,\n",
       " 'Можга': 213,\n",
       " 'Иглино': 214,\n",
       " 'Киселевск': 215,\n",
       " 'Веселый': 216,\n",
       " 'Городец': 217,\n",
       " 'Саянск': 218,\n",
       " 'Реж': 219,\n",
       " 'Добрянка': 220,\n",
       " 'Старый Оскол': 221,\n",
       " 'Аксай': 222,\n",
       " 'Ровеньки': 223,\n",
       " 'Фролово': 224,\n",
       " 'Югорск': 225,\n",
       " 'Татарка': 226,\n",
       " 'Воронежская': 227,\n",
       " 'Губкин': 228,\n",
       " 'Новокуйбышевск': 229,\n",
       " 'Снежинск': 230,\n",
       " 'Железногорск': 231,\n",
       " 'Богородск': 232,\n",
       " 'Донецк': 233,\n",
       " 'Бор': 234,\n",
       " 'Лениногорск': 235,\n",
       " 'Новая Усмань': 236,\n",
       " 'Батайск': 237,\n",
       " 'Алексеевка': 238,\n",
       " 'Железногорск-Илимский': 239,\n",
       " 'Михайловка': 240,\n",
       " 'Краснокамск': 241,\n",
       " 'Аткарск': 242,\n",
       " 'Зеленокумск': 243,\n",
       " 'Юрюзань': 244,\n",
       " 'Краснознаменск': 245,\n",
       " 'Туймазы': 246,\n",
       " 'Борисовка': 247,\n",
       " 'Новочеркасск': 248,\n",
       " 'Оса': 249,\n",
       " 'Судогда': 250,\n",
       " 'Кытманово': 251,\n",
       " 'Жирновск': 252,\n",
       " 'Нехаевская': 253,\n",
       " 'Покров': 254,\n",
       " 'Бугульма': 255,\n",
       " 'Архипо-Осиповка': 256,\n",
       " 'Соль-Илецк': 257,\n",
       " 'Когалым': 258,\n",
       " 'Артемовский': 259,\n",
       " 'Балашов': 260,\n",
       " 'Красногвардейское': 261,\n",
       " 'Новошахтинск': 262,\n",
       " 'Кунгур': 263,\n",
       " 'Выселки': 264,\n",
       " 'Урай': 265,\n",
       " 'Пугачев': 266,\n",
       " 'Борское': 267,\n",
       " 'Златоуст': 268,\n",
       " 'Калач': 269,\n",
       " 'Бирск': 270,\n",
       " 'Раевский': 271,\n",
       " 'Белев': 272,\n",
       " 'Белая Калитва': 273,\n",
       " 'Мостовской': 274,\n",
       " 'Северская': 275,\n",
       " 'Куйбышев': 276,\n",
       " 'Заречный': 277,\n",
       " 'Белореченск': 278,\n",
       " 'Кушва': 279,\n",
       " 'Гуково': 280,\n",
       " 'Кабардинка': 281,\n",
       " 'Казанское': 282,\n",
       " 'Топки': 283,\n",
       " 'Карасук': 284,\n",
       " 'Советский': 285,\n",
       " 'Петра Дубрава': 286,\n",
       " 'Тулун': 287,\n",
       " 'Муслюмово': 288,\n",
       " 'Сибай': 289,\n",
       " 'Усть-Лабинск': 290,\n",
       " 'Дивногорск': 291,\n",
       " 'Старощербиновская': 292,\n",
       " 'Красный Сулин': 293,\n",
       " 'Анжеро-Судженск': 294,\n",
       " 'Боргустанская': 295,\n",
       " 'Изобильный': 296,\n",
       " 'Апшеронск': 297,\n",
       " 'Новокубанск': 298,\n",
       " 'Лысьва': 299,\n",
       " 'Посевная': 300,\n",
       " 'Большое Село': 301,\n",
       " 'Аша': 302,\n",
       " 'Давыдовка': 303,\n",
       " 'Ильский': 304,\n",
       " 'Богучар': 305,\n",
       " 'Приморско-Ахтарск': 306,\n",
       " 'Ордынское': 307,\n",
       " 'Киреевск': 308,\n",
       " 'Багаевская': 309,\n",
       " 'Заволжье': 310,\n",
       " 'Плавск': 311,\n",
       " 'Брюховецкая': 312,\n",
       " 'Ишимбай': 313,\n",
       " 'Рудня': 314,\n",
       " 'Ирбит': 315,\n",
       " 'Лукоянов': 316,\n",
       " 'Озерск': 317,\n",
       " 'Янтарный': 318,\n",
       " 'Воробьевка': 319,\n",
       " 'Новодмитриевская': 320,\n",
       " 'Агрыз': 321,\n",
       " 'Правдинск': 322,\n",
       " 'Ростов': 323,\n",
       " 'Дубовое': 324,\n",
       " 'Семилуки': 325,\n",
       " 'Усть-Кишерть': 326,\n",
       " 'Елабуга': 327,\n",
       " 'Кимовск': 328,\n",
       " 'Мариинск': 329,\n",
       " 'Большой Исток': 330,\n",
       " 'Аскино': 331,\n",
       " 'Барабинск': 332,\n",
       " 'Каменск-Шахтинский': 333,\n",
       " 'Славгород': 334,\n",
       " 'Зерноград': 335,\n",
       " 'Волоконовка': 336,\n",
       " 'Маркс': 337,\n",
       " 'Назарово': 338,\n",
       " 'Анапская': 339,\n",
       " 'Меленки': 340,\n",
       " 'Черлак': 341,\n",
       " 'Петровск': 342,\n",
       " 'Балахна': 343,\n",
       " 'Красные Ткачи': 344,\n",
       " 'Новый Городок': 345,\n",
       " 'Нововоронеж': 346,\n",
       " 'Кочубеевское': 347,\n",
       " 'Полетаево': 348,\n",
       " 'Тайга': 349,\n",
       " 'Александровское': 350,\n",
       " 'Сатка': 351,\n",
       " 'Воткинск': 352,\n",
       " 'Гусев': 353,\n",
       " 'Нытва': 354,\n",
       " 'Нижние Серги': 355,\n",
       " 'Лянтор': 356,\n",
       " 'Сеченово': 357,\n",
       " 'Туапсе': 358,\n",
       " 'Отрадная': 359,\n",
       " 'Кез': 360,\n",
       " 'Чкаловск': 361,\n",
       " 'Арзгир': 362,\n",
       " 'Сорочинск': 363,\n",
       " 'Поспелиха': 364,\n",
       " 'Ленинградская': 365,\n",
       " 'Бирюч': 366,\n",
       " 'Новотроицк': 367,\n",
       " 'Княгинино': 368,\n",
       " 'Лиски': 369,\n",
       " 'Крымск': 370,\n",
       " 'Калач-на-Дону': 371,\n",
       " 'Верхнебаканский': 372,\n",
       " 'Котельниково': 373,\n",
       " 'Красноуральск': 374,\n",
       " 'Ленинск-Кузнецкий': 375,\n",
       " 'Чусовой': 376,\n",
       " 'Горьковское': 377,\n",
       " 'Большая Черниговка': 378,\n",
       " 'Лесной': 379,\n",
       " 'Ярославская': 380,\n",
       " 'Бодайбо': 381,\n",
       " 'Новопавловск': 382,\n",
       " 'Называевск': 383,\n",
       " 'Кыштым': 384,\n",
       " 'Ершов': 385,\n",
       " 'Черняховск': 386,\n",
       " 'Сухой Лог': 387,\n",
       " 'Вышестеблиевская': 388,\n",
       " 'Жигулевск': 389,\n",
       " 'Семикаракорск': 390,\n",
       " 'Тамань': 391,\n",
       " 'Азнакаево': 392,\n",
       " 'Светлоград': 393,\n",
       " 'Яйва': 394,\n",
       " 'Нижнеудинск': 395,\n",
       " 'Нижнебаканская': 396,\n",
       " 'Заокский': 397,\n",
       " 'Нефтегорск': 398,\n",
       " 'Камышлов': 399,\n",
       " 'Петушки': 400,\n",
       " 'Кондратово': 401,\n",
       " 'Арсеньево': 402,\n",
       " 'Балезино': 403,\n",
       " 'Григорополисская': 404,\n",
       " 'Невьянск': 405,\n",
       " 'Тюльган': 406,\n",
       " 'Новоалександровск': 407,\n",
       " 'Талнах': 408,\n",
       " 'Ленинский': 409,\n",
       " 'Междуреченский': 410,\n",
       " 'Птичье': 411,\n",
       " 'Туринск': 412,\n",
       " 'Воротынец': 413,\n",
       " 'Любим': 414,\n",
       " 'Мыски': 415,\n",
       " 'Филимоново': 416,\n",
       " 'Ефремов': 417,\n",
       " 'Богданович': 418,\n",
       " 'Пыть-Ях': 419,\n",
       " 'Таштагол': 420,\n",
       " 'Камень-на-Оби': 421,\n",
       " 'Зеленоградск': 422,\n",
       " 'Куса': 423,\n",
       " 'Ясногорск': 424,\n",
       " 'Кантемировка': 425,\n",
       " 'Осинники': 426,\n",
       " 'Краснообск': 427,\n",
       " 'Вагай': 428,\n",
       " 'Пильна': 429,\n",
       " 'Сюмси': 430,\n",
       " 'Ворсма': 431,\n",
       " 'Ленинск': 432,\n",
       " 'Аскарово': 433,\n",
       " 'Верхняя Тишанка': 434,\n",
       " 'Тавда': 435,\n",
       " 'Суксун': 436,\n",
       " 'Арти': 437,\n",
       " 'Урень': 438,\n",
       " 'Енисейск': 439,\n",
       " 'Ладушкин': 440,\n",
       " 'Тутаев': 441,\n",
       " 'Черемхово': 442,\n",
       " 'Малая Пурга': 443,\n",
       " 'Балтийск': 444,\n",
       " 'Тбилисская': 445,\n",
       " 'Ахтырский': 446,\n",
       " 'Старовеличковская': 447,\n",
       " 'Аргаяш': 448,\n",
       " 'Витязево': 449,\n",
       " 'Полтавская': 450,\n",
       " 'Уват': 451,\n",
       " 'Саров': 452,\n",
       " 'Тяжинский': 453,\n",
       " 'Двуреченск': 454,\n",
       " 'Усть-Донецкий': 455,\n",
       " 'Красноусольский': 456,\n",
       " 'Калининская': 457,\n",
       " 'Алнаши': 458,\n",
       " 'Муромцево': 459,\n",
       " 'Высокая Гора': 460,\n",
       " 'Новотроицкая': 461,\n",
       " 'Оконешниково': 462,\n",
       " 'Тальменка': 463,\n",
       " 'Любинский': 464,\n",
       " 'Усть-Кут': 465,\n",
       " 'Южный': 466,\n",
       " 'Агидель': 467,\n",
       " 'Федоровский': 468,\n",
       " 'Разумное': 469,\n",
       " 'Большие Чапурники': 470,\n",
       " 'Дегтярск': 471,\n",
       " 'Нижняя Салда': 472,\n",
       " 'Мышкин': 473,\n",
       " 'Очер': 474,\n",
       " 'Суворов': 475,\n",
       " 'Бутурлиновка': 476,\n",
       " 'Тоцкое': 477,\n",
       " 'Сергач': 478,\n",
       " 'Средняя Ахтуба': 479,\n",
       " 'Приволжье': 480,\n",
       " 'Переясловская': 481,\n",
       " 'Красноуфимск': 482,\n",
       " 'Пролетарск': 483,\n",
       " 'Белоярский': 484,\n",
       " 'Сысерть': 485,\n",
       " 'Черепаново': 486,\n",
       " 'Уральский': 487,\n",
       " 'Вейделевка': 488,\n",
       " 'Терновка': 489,\n",
       " 'Азово': 490,\n",
       " 'Алтайское': 491,\n",
       " 'Собинка': 492,\n",
       " 'Ольховатка': 493,\n",
       " 'Кашары': 494,\n",
       " 'Гороховец': 495,\n",
       " 'Пестрецы': 496,\n",
       " 'Приютово': 497,\n",
       " 'Каневская': 498,\n",
       " 'Верхнеуральск': 499,\n",
       " 'Динская': 500,\n",
       " 'Колывань': 501,\n",
       " 'Курсавка': 502,\n",
       " 'Сергиевск': 503,\n",
       " 'Костерево': 504,\n",
       " 'Медногорск': 505,\n",
       " 'Обь': 506,\n",
       " 'Исилькуль': 507,\n",
       " 'Зима': 508,\n",
       " 'Лакинск': 509,\n",
       " 'Нефтекумск': 510,\n",
       " 'Знаменск': 511,\n",
       " 'Иланский': 512,\n",
       " 'Усть-Кинельский': 513,\n",
       " 'Нижняя Тавда': 514,\n",
       " 'Кизнер': 515,\n",
       " 'Красная Горбатка': 516,\n",
       " 'Верхние Татышлы': 517,\n",
       " 'Верхний Услон': 518,\n",
       " 'Катав-Ивановск': 519,\n",
       " 'Новомихайловский кп': 520,\n",
       " 'Домбаровский': 521,\n",
       " 'Лузино': 522,\n",
       " 'Камбарка': 523,\n",
       " 'Линево': 524,\n",
       " 'Светлый': 525,\n",
       " 'Полазна': 526,\n",
       " 'Шаркан': 527,\n",
       " 'Новоукраинское': 528,\n",
       " 'Калачинск': 529,\n",
       " 'Кореновск': 530,\n",
       " 'Калининск': 531,\n",
       " 'Белый Яр': 532,\n",
       " 'Солнечнодольск': 533,\n",
       " 'Радужный': 534,\n",
       " 'Верхнеяркеево': 535,\n",
       " 'Актаныш': 536,\n",
       " 'Пластуновская': 537,\n",
       " 'Байкалово': 538,\n",
       " 'Большая Мурта': 539,\n",
       " 'Благовещенск': 540,\n",
       " 'Александровск': 541,\n",
       " 'Углич': 542,\n",
       " 'Красный Кут': 543,\n",
       " 'Дудинка': 544,\n",
       " 'Ермаковское': 545,\n",
       " 'Еманжелинск': 546,\n",
       " 'Бакал': 547,\n",
       " 'Короча': 548,\n",
       " 'Кагальницкая': 549,\n",
       " 'Неман': 550,\n",
       " 'Струнино': 551,\n",
       " 'Ивантеевка': 552,\n",
       " 'Заинск': 553,\n",
       " 'Ладожская': 554,\n",
       " 'Богородицк': 555,\n",
       " 'Матвеев-Курган': 556,\n",
       " 'Звездный': 557,\n",
       " 'Павловск': 558,\n",
       " 'Зимовники': 559,\n",
       " 'Безенчук': 560,\n",
       " 'Новошешминск': 561,\n",
       " 'Чишмы': 562,\n",
       " 'Кинель-Черкассы': 563,\n",
       " 'Гремячинск': 564,\n",
       " 'Чик': 565,\n",
       " 'Новониколаевский': 566,\n",
       " 'Шушенское': 567,\n",
       " 'Култаево': 568,\n",
       " 'Ольховка': 569,\n",
       " 'Нижнесортымский': 570,\n",
       " 'Кизел': 571,\n",
       " 'Адагум': 572,\n",
       " 'Левокумское': 573,\n",
       " 'Шелехов': 574,\n",
       " 'Семенов': 575,\n",
       " 'Сальск': 576,\n",
       " 'Байкальск': 577,\n",
       " 'Некрасовское': 578,\n",
       " 'Валуйки': 579,\n",
       " 'Октябрьская': 580,\n",
       " 'Летняя Ставка': 581,\n",
       " 'Козулька': 582,\n",
       " 'Пионерский': 583,\n",
       " 'Смоленская': 584,\n",
       " 'Кедровый': 585,\n",
       " 'Шелаболиха': 586,\n",
       " 'Быково': 587,\n",
       " 'Томаровка': 588,\n",
       " 'Донской': 589,\n",
       " 'Курманаевка': 590,\n",
       " 'Савинка': 591,\n",
       " 'Кудымкар': 592,\n",
       " 'Крыловская': 593,\n",
       " 'Новоаннинский': 594,\n",
       " 'Нижняя Тура': 595,\n",
       " 'Тугулым': 596,\n",
       " 'Первомайский': 597,\n",
       " 'Рамонь': 598,\n",
       " 'Николо-Павловское': 599,\n",
       " 'Красный Яр': 600,\n",
       " 'Кольцово': 601,\n",
       " 'Пятигорский': 602,\n",
       " 'Варна': 603,\n",
       " 'Алейск': 604,\n",
       " 'Талица': 605,\n",
       " 'Венев': 606,\n",
       " 'Селты': 607,\n",
       " 'Лангепас': 608,\n",
       " 'Железноводск': 609,\n",
       " 'Черноморский': 610,\n",
       " 'Воскресенское': 611,\n",
       " 'Курагино': 612,\n",
       " 'Большой Лог': 613,\n",
       " 'Иноземцево кп': 614,\n",
       " 'Алапаевск': 615,\n",
       " 'Светлый Яр': 616,\n",
       " 'Вихоревка': 617,\n",
       " 'Переволоцкий': 618,\n",
       " 'Промышленная': 619,\n",
       " 'Сибирский': 620,\n",
       " 'Троицкое': 621,\n",
       " 'Краснослободск': 622,\n",
       " 'Вешенская': 623,\n",
       " 'Палласовка': 624,\n",
       " 'Старотитаровская': 625,\n",
       " 'Острогожск': 626,\n",
       " 'Стародеревянковская': 627,\n",
       " 'Бутурлино': 628,\n",
       " 'Качуг': 629,\n",
       " 'Мошково': 630,\n",
       " 'Покачи': 631,\n",
       " 'Новопокровская': 632,\n",
       " 'Агаповка': 633,\n",
       " 'Кандры': 634,\n",
       " 'Кулешовка': 635,\n",
       " 'Обливская': 636,\n",
       " 'Красное': 637,\n",
       " 'Яровое': 638,\n",
       " 'Чарышское': 639,\n",
       " 'Бехтеевка': 640,\n",
       " 'Подтесово': 641,\n",
       " 'Даниловка': 642,\n",
       " 'Котово': 643,\n",
       " 'Рыбная Слобода': 644,\n",
       " 'Вишневогорск': 645,\n",
       " 'Степное': 646,\n",
       " 'Богандинский': 647,\n",
       " 'Вача': 648,\n",
       " 'Стрелка': 649,\n",
       " 'Отрадное': 650,\n",
       " 'Дубовка': 651,\n",
       " 'Исетское': 652,\n",
       " 'Данилов': 653,\n",
       " 'Бавлы': 654,\n",
       " 'Старомарьевка': 655,\n",
       " 'Кольчугино': 656,\n",
       " 'Шварцевский': 657,\n",
       " 'Богучаны': 658,\n",
       " 'Починки': 659,\n",
       " 'Губаха': 660,\n",
       " 'Гвардейск': 661,\n",
       " 'Новоблагодарное': 662,\n",
       " 'Еткуль': 663,\n",
       " 'Верхний Уфалей': 664,\n",
       " 'Николаевск': 665,\n",
       " 'Балакирево': 666,\n",
       " 'Новый Некоуз': 667,\n",
       " 'Голышманово': 668,\n",
       " 'Татарск': 669,\n",
       " 'Семибратово': 670,\n",
       " 'Бекешевская': 671,\n",
       " 'Здвинск': 672,\n",
       " 'Юганец': 673,\n",
       " 'Чертково': 674,\n",
       " 'Петропавловка': 675,\n",
       " 'Заветное': 676,\n",
       " 'Шарлык': 677,\n",
       " 'Суроватиха': 678,\n",
       " 'Гаврилов-Ям': 679,\n",
       " 'Зеленогорский': 680,\n",
       " 'Иловля': 681,\n",
       " 'Ува': 682,\n",
       " 'Шахунья': 683,\n",
       " 'Подгоренский': 684,\n",
       " 'Новосергиевка': 685,\n",
       " 'Сокольники': 686,\n",
       " 'Горняк': 687,\n",
       " 'Лесосибирск': 688,\n",
       " 'Дорогино': 689,\n",
       " 'Большое Сорокино': 690,\n",
       " 'Боровиха': 691,\n",
       " 'Прохоровка': 692,\n",
       " 'Омутинское': 693,\n",
       " 'Тарасовский': 694,\n",
       " 'Суворовская': 695,\n",
       " 'Юрьев-Польский': 696,\n",
       " 'Трехгорный': 697,\n",
       " 'Караидель': 698,\n",
       " 'Эдиссия': 699,\n",
       " 'Ивдель': 700,\n",
       " 'Поворино': 701,\n",
       " 'Марьяновка': 702,\n",
       " 'Кумылженская': 703,\n",
       " 'Красная Яруга': 704,\n",
       " 'Абрау-Дюрсо': 705,\n",
       " 'Вавож': 706,\n",
       " 'Слюдянка': 707,\n",
       " 'Красногорское': 708,\n",
       " 'Новомышастовская': 709,\n",
       " 'Цимлянск': 710,\n",
       " 'Курская': 711,\n",
       " 'Среднеуральск': 712,\n",
       " 'Буздяк': 713,\n",
       " 'Авдон': 714,\n",
       " 'Пласт': 715,\n",
       " 'Юкаменское': 716,\n",
       " 'Суровикино': 717,\n",
       " 'Мегет': 718,\n",
       " 'Уйское': 719,\n",
       " 'Новотитаровская': 720,\n",
       " 'Топчиха': 721,\n",
       " 'Кунашак': 722,\n",
       " 'Зилаир': 723,\n",
       " 'Тоцкое Второе': 724,\n",
       " 'Бородино': 725,\n",
       " 'Шатки': 726,\n",
       " 'Рассвет': 727,\n",
       " 'Маркова': 728,\n",
       " 'Полысаево': 729,\n",
       " 'Старолеушковская': 730,\n",
       " 'Выездное': 731,\n",
       " 'Свирск': 732,\n",
       " 'Вольгинский': 733,\n",
       " 'Абан': 734,\n",
       " 'Белая Глина': 735,\n",
       " 'Бураево': 736,\n",
       " 'Головчино': 737,\n",
       " 'Кармаскалы': 738,\n",
       " 'Новосемейкино': 739,\n",
       " 'Новоминская': 740,\n",
       " 'Одесское': 741,\n",
       " 'Абдулино': 742,\n",
       " 'Бобров': 743,\n",
       " 'Североуральск': 744,\n",
       " 'Пригородный': 745,\n",
       " 'Рефтинский': 746,\n",
       " 'Шерегеш': 747,\n",
       " 'Сростки': 748,\n",
       " 'Старомышастовская': 749,\n",
       " 'Джубга кп': 750,\n",
       " 'Большеустьикинское': 751,\n",
       " 'Черемшан': 752,\n",
       " 'Емельяново': 753,\n",
       " 'Исянгулово': 754,\n",
       " 'Боготол': 755,\n",
       " 'Ессентукская': 756,\n",
       " 'Небуг': 757,\n",
       " 'Алексеевская': 758,\n",
       " 'Белореченский': 759,\n",
       " 'Репьевка': 760,\n",
       " 'Новокорсунская': 761,\n",
       " 'Кузино': 762,\n",
       " 'Апастово': 763,\n",
       " 'Песчанокопское': 764,\n",
       " 'Грибановский': 765,\n",
       " 'Калманка': 766,\n",
       " 'Баймак': 767,\n",
       " 'Шаран': 768,\n",
       " 'Излучинск': 769,\n",
       " 'Эртиль': 770,\n",
       " 'Купино': 771,\n",
       " 'Дубовское': 772,\n",
       " 'Винзили': 773,\n",
       " 'Чесма': 774,\n",
       " 'Алексеевское': 775,\n",
       " 'Андреево': 776,\n",
       " 'Кабаково': 777,\n",
       " 'Первомайск': 778,\n",
       " 'Волчанск': 779,\n",
       " 'Тетюши': 780,\n",
       " 'Серафимовский': 781,\n",
       " 'Саракташ': 782,\n",
       " 'Зауральский': 783,\n",
       " 'Октябрьск': 784,\n",
       " 'Болотное': 785,\n",
       " 'Донское': 786,\n",
       " 'Суздаль': 787,\n",
       " 'Ртищево': 788,\n",
       " 'Натухаевская': 789,\n",
       " 'Грахово': 790,\n",
       " 'Петров Вал': 791,\n",
       " 'Викулово': 792,\n",
       " 'Глубокий': 793,\n",
       " 'Пелагиада': 794,\n",
       " 'Чернышковский': 795,\n",
       " 'Новогуровский': 796,\n",
       " 'Романовская': 797,\n",
       " 'Гай': 798,\n",
       " 'Этока': 799,\n",
       " 'Совхозный': 800,\n",
       " 'Перевоз': 801,\n",
       " 'Одоев': 802,\n",
       " 'Красная Поляна': 803,\n",
       " 'Благодарный': 804,\n",
       " 'Малиновое Озеро': 805,\n",
       " 'Вознесенское': 806,\n",
       " 'Прибрежный': 807,\n",
       " 'Лермонтов': 808,\n",
       " 'Нововеличковская': 809,\n",
       " 'Верещагино': 810,\n",
       " 'Октябрьское': 811,\n",
       " 'Большое Козино': 812,\n",
       " 'Кошурниково': 813,\n",
       " 'Озерки': 814,\n",
       " 'Васюринская': 815,\n",
       " 'Панино': 816,\n",
       " 'Кедровка': 817,\n",
       " 'Сим': 818,\n",
       " 'Оек': 819,\n",
       " 'Салым': 820,\n",
       " 'Новосмолинский': 821,\n",
       " 'Успенское': 822,\n",
       " 'Ростовка': 823,\n",
       " 'Хохольский': 824,\n",
       " 'Мензелинск': 825,\n",
       " 'Петропавловская': 826,\n",
       " 'Городище': 827,\n",
       " 'Малиновка': 828,\n",
       " 'Покровское': 829,\n",
       " 'Майский': 830,\n",
       " 'Тонкино': 831,\n",
       " 'Менделеевск': 832,\n",
       " 'Удобная': 833,\n",
       " 'Северо-Задонск': 834,\n",
       " 'Новолеушковская': 835,\n",
       " 'Курчанская': 836,\n",
       " 'Березовка': 837,\n",
       " 'Акбулак': 838,\n",
       " 'Новохоперск': 839,\n",
       " 'Полесск': 840,\n",
       " 'Полтавка': 841,\n",
       " 'Верхотурье': 842,\n",
       " 'Арья': 843,\n",
       " 'Пышма': 844,\n",
       " 'Елизаветинская': 845,\n",
       " 'Чалтырь': 846,\n",
       " 'Кавказская': 847,\n",
       " 'Месягутово': 848,\n",
       " 'Труновское': 849,\n",
       " 'Питерка': 850,\n",
       " 'Липки': 851,\n",
       " 'Георгиевская': 852,\n",
       " 'Большое Болдино': 853,\n",
       " 'Красноармейск': 854,\n",
       " 'Карагай': 855,\n",
       " 'Бородинский': 856,\n",
       " 'Тюбук': 857,\n",
       " 'Чаны': 858,\n",
       " 'Долгодеревенское': 859,\n",
       " 'Егорлыкская': 860,\n",
       " 'Касли': 861,\n",
       " 'Новоселицкое': 862,\n",
       " 'Лукино': 863,\n",
       " 'Чермоз': 864,\n",
       " 'Светлогорск': 865,\n",
       " 'Дальнее Константиново': 866,\n",
       " 'Арск': 867,\n",
       " 'Ильинский': 868,\n",
       " 'Тевриз': 869,\n",
       " 'Чунский': 870,\n",
       " 'Приобье': 871,\n",
       " 'Аксубаево': 872,\n",
       " 'Красногорский': 873,\n",
       " 'Кировград': 874,\n",
       " 'Мишкино': 875,\n",
       " 'Новолуговое': 876,\n",
       " 'Новый Буян': 877,\n",
       " 'Зеленогорск': 878,\n",
       " 'Кушнаренково': 879,\n",
       " 'Верхняя Тура': 880,\n",
       " 'Баево': 881,\n",
       " 'Дивное': 882,\n",
       " 'Глебовка': 883,\n",
       " 'Березанская': 884,\n",
       " 'Заплавное': 885,\n",
       " 'Богатое': 886,\n",
       " 'Славск': 887,\n",
       " 'Зверево': 888,\n",
       " 'Пошехонье': 889,\n",
       " 'Ракитное': 890,\n",
       " 'Калужская': 891,\n",
       " 'Никологоры': 892,\n",
       " 'Каратузское': 893,\n",
       " 'Уруссу': 894,\n",
       " 'Верхний Тагил': 895,\n",
       " 'Ивня': 896,\n",
       " 'Ардатов': 897,\n",
       " 'Краснотуранск': 898,\n",
       " 'Миньяр': 899,\n",
       " 'Ижморский': 900,\n",
       " 'Крапивинский': 901,\n",
       " 'Самарское': 902,\n",
       " 'Боголюбово': 903,\n",
       " 'Роза': 904,\n",
       " 'Карабаново': 905,\n",
       " 'Успенская': 906,\n",
       " 'Таловая': 907,\n",
       " 'Ярково': 908,\n",
       " 'Камешково': 909,\n",
       " 'Калтан': 910,\n",
       " 'Родино': 911,\n",
       " 'Юго-Камский': 912,\n",
       " 'Яя': 913,\n",
       " 'Чекмагуш': 914,\n",
       " 'Мраково': 915,\n",
       " 'Рощинский': 916,\n",
       " 'Холмская': 917,\n",
       " 'Целина': 918,\n",
       " 'Идринское': 919,\n",
       " 'Грицовский': 920,\n",
       " 'Анна': 921,\n",
       " 'Краснозерское': 922,\n",
       " 'Лесная Поляна': 923,\n",
       " 'Аркадак': 924,\n",
       " 'Новобелокатай': 925,\n",
       " 'Теплое': 926,\n",
       " 'Староалейское': 927,\n",
       " 'Подкумок': 928,\n",
       " 'Кукуштан': 929,\n",
       " 'Мелехово': 930,\n",
       " 'Привольная': 931,\n",
       " 'Ермолаево': 932,\n",
       " 'Грачевка': 933,\n",
       " 'Новоселезнево': 934,\n",
       " 'Бессоновка': 935,\n",
       " 'Акъяр': 936,\n",
       " 'Коченево': 937,\n",
       " 'Константиновское': 938,\n",
       " 'Нижняя Мактама': 939,\n",
       " 'Криводановка': 940,\n",
       " 'Советская': 941,\n",
       " 'Малояз': 942,\n",
       " 'Надежда': 943,\n",
       " 'Плотниково': 944,\n",
       " 'Куеда': 945,\n",
       " 'Первомайское': 946,\n",
       " 'Усть-Ордынский': 947,\n",
       " 'Горнозаводск': 948,\n",
       " 'Илек': 949,\n",
       " 'Нязепетровск': 950,\n",
       " 'Петровская': 951,\n",
       " 'Стройкерамика': 952,\n",
       " 'Грайворон': 953,\n",
       " 'Васильево': 954,\n",
       " 'Большая Соснова': 955,\n",
       " 'Осиново': 956,\n",
       " 'Гривенская': 957,\n",
       " 'Красные Баки': 958,\n",
       " 'Кировская': 959,\n",
       " 'Яшкино': 960,\n",
       " 'Джалиль': 961,\n",
       " 'Хомутово': 962,\n",
       " 'Благовещенка': 963,\n",
       " 'Орловский': 964,\n",
       " 'Балахта': 965,\n",
       " 'Краснощеково': 966,\n",
       " 'Яр': 967,\n",
       " 'Тимирязевский': 968,\n",
       " 'Львовское': 969,\n",
       " 'Приморка': 970,\n",
       " 'Жирнов': 971,\n",
       " 'Большая Глушица': 972,\n",
       " 'Бреды': 973,\n",
       " 'Баган': 974,\n",
       " 'Миасское': 975,\n",
       " 'Заозерный': 976,\n",
       " 'Терновская': 977,\n",
       " 'Завьялово': 978,\n",
       " 'Армизонское': 979,\n",
       " 'Марьянская': 980,\n",
       " 'Карабаш': 981,\n",
       " 'Новобатайск': 982,\n",
       " 'Саваслейка': 983,\n",
       " 'Архангельское': 984,\n",
       " 'Ставрово': 985,\n",
       " 'Волово': 986,\n",
       " 'Тацинская': 987,\n",
       " 'Архангельская': 988,\n",
       " 'Лобва': 989,\n",
       " 'Борисоглебский': 990,\n",
       " 'Шигоны': 991,\n",
       " 'Барда': 992,\n",
       " 'Голубицкая': 993,\n",
       " 'Залари': 994,\n",
       " 'Сарманово': 995,\n",
       " 'Ильиногорск': 996,\n",
       " 'Бакалы': 997,\n",
       " 'Затеречный': 998,\n",
       " 'Хвалынск': 999,\n",
       " 'Пономарёвка': 1000,\n",
       " ...}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "city_tk =  {x:i+1 for i, x in enumerate(train_df.city.unique())}\n",
    "city_tk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1277910,)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_city   = np.asarray([city_tk.get(key, 0) for key in train_df.city], dtype=int)\n",
    "#len(x_city)\n",
    "x_city.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'input_3:0' shape=(?, 1) dtype=float32>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_size = 32\n",
    "inp_city = Input(shape=(1,))\n",
    "emb_city = Embedding(len(city_tk)+1, emb_size)(inp_city)\n",
    "emb_city\n",
    "inp_city"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dhiman\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:47: RuntimeWarning: divide by zero encountered in log1p\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "##================Create the Tokenizers\n",
    "region_tk = {x:i+1 for i, x in enumerate(train_df.region.unique())}#+1 because we want to reserve 0 for new but not missing values\n",
    "city_tk =  {x:i+1 for i, x in enumerate(train_df.city.unique())}\n",
    "cat1_tk =  {x:i+1 for i, x in enumerate(train_df.parent_category_name.unique())}\n",
    "cat2_tk =  {x:i+1 for i, x in enumerate(train_df.category_name.unique())}\n",
    "param1_tk =  {x:i+1 for i, x in enumerate(train_df.param_1.unique())}\n",
    "param2_tk =  {x:i+1 for i, x in enumerate(train_df.param_2.unique())}\n",
    "param3_tk =  {x:i+1 for i, x in enumerate(train_df.param_3.unique())}\n",
    "seqnum_tk =  {x:i+1 for i, x in enumerate(train_df.item_seq_number.unique())}\n",
    "usertype_tk = {x:i+1 for i, x in enumerate(train_df.user_type.unique())}\n",
    "imgtype_tk = {x:i+1 for i, x in enumerate(train_df.image_top_1.unique())}\n",
    "tokenizers = [region_tk, city_tk, cat1_tk, cat2_tk, param1_tk, param2_tk, param3_tk, seqnum_tk, usertype_tk, imgtype_tk]\n",
    "\n",
    "##================These functions are going to get repeated on train, val, and test data\n",
    "'''def tokenize_data(data, tokenizers):\n",
    "    region_tk, city_tk, cat1_tk, cat2_tk, param1_tk, param2_tk, param3_tk, seqnum_tk, usertype_tk, imgtype_tk = tokenizers\n",
    "    x_reg = np.asarray([region_tk.get(key, 0) for key in data.region], dtype=int)\n",
    "    x_city   = np.asarray([city_tk.get(key, 0) for key in data.city], dtype=int)\n",
    "    x_cat1   = np.asarray([cat1_tk.get(key, 0) for key in data.parent_category_name], dtype=int)\n",
    "    x_cat2   = np.asarray([cat2_tk.get(key, 0) for key in data.category_name], dtype=int)\n",
    "    x_prm1 = np.asarray([param1_tk.get(key, 0) for key in data.param_1], dtype=int)\n",
    "    x_prm2 = np.asarray([param2_tk.get(key, 0) for key in data.param_2], dtype=int)\n",
    "    x_prm3 = np.asarray([param3_tk.get(key, 0) for key in data.param_3], dtype=int)\n",
    "    x_sqnm = np.asarray([seqnum_tk.get(key, 0) for key in data.item_seq_number], dtype=int)\n",
    "    x_usr = np.asarray([usertype_tk.get(key, 0) for key in data.user_type], dtype=int)\n",
    "    x_itype = np.asarray([imgtype_tk.get(key, 0) for key in data.image_top_1], dtype=int)\n",
    "    return [x_reg, x_city, x_cat1, x_cat2, x_prm1, x_prm2, x_prm3, x_sqnm, x_usr, x_itype]\n",
    "'''\n",
    "##================These functions are going to get repeated on train, val, and test data\n",
    "def tokenize_data(trn_data,test_data):\n",
    "    \n",
    "    x_reg = np.asarray([region_tk.get(key, 0) for key in data.region], dtype=int)\n",
    "    x_city   = np.asarray([city_tk.get(key, 0) for key in data.city], dtype=int)\n",
    "    x_cat1   = np.asarray([cat1_tk.get(key, 0) for key in data.parent_category_name], dtype=int)\n",
    "    x_cat2   = np.asarray([cat2_tk.get(key, 0) for key in data.category_name], dtype=int)\n",
    "    x_prm1 = np.asarray([param1_tk.get(key, 0) for key in data.param_1], dtype=int)\n",
    "    x_prm2 = np.asarray([param2_tk.get(key, 0) for key in data.param_2], dtype=int)\n",
    "    x_prm3 = np.asarray([param3_tk.get(key, 0) for key in data.param_3], dtype=int)\n",
    "    x_sqnm = np.asarray([seqnum_tk.get(key, 0) for key in data.item_seq_number], dtype=int)\n",
    "    x_usr = np.asarray([usertype_tk.get(key, 0) for key in data.user_type], dtype=int)\n",
    "    x_itype = np.asarray([imgtype_tk.get(key, 0) for key in data.image_top_1], dtype=int)\n",
    "    return [x_reg, x_city, x_cat1, x_cat2, x_prm1, x_prm2, x_prm3, x_sqnm, x_usr, x_itype]\n",
    "\n",
    "def log_prices(data):\n",
    "    prices = data.price.as_matrix()\n",
    "    prices = np.log1p(prices)\n",
    "    prices[prices==-np.inf] = -1\n",
    "    return prices\n",
    "\n",
    "##================Final Processing on x, y train, val, test data\n",
    "x_train = tokenize_data(train_df, tokenizers)\n",
    "x_train.append(train_df.activation_date.as_matrix())\n",
    "x_train.append(log_prices(train_df))\n",
    "y_train = train_df.deal_probability.as_matrix()\n",
    "\n",
    "x_val = tokenize_data(val_df, tokenizers)\n",
    "x_val.append(val_df.activation_date.as_matrix())\n",
    "x_val.append(log_prices(val_df))\n",
    "y_val = val_df.deal_probability.as_matrix()\n",
    "\n",
    "x_test = tokenize_data(test_df, tokenizers)\n",
    "x_test.append(test_df.activation_date.as_matrix())\n",
    "x_test.append(log_prices(test_df))\n",
    "\n",
    "##================Beginning of the NN Model Outline. \n",
    "def build_model():\n",
    "    inp_reg = Input(shape=(1,))\n",
    "    inp_city = Input(shape=(1,))\n",
    "    inp_cat1 = Input(shape=(1,))\n",
    "    inp_cat2 = Input(shape=(1,))\n",
    "    inp_prm1 = Input(shape=(1,))\n",
    "    inp_prm2 = Input(shape=(1,))\n",
    "    inp_prm3 = Input(shape=(1,))\n",
    "    inp_sqnm = Input(shape=(1,))\n",
    "    inp_usr = Input(shape=(1,))\n",
    "    inp_itype = Input(shape=(1,))\n",
    "    inp_weekday = Input(shape=(1,))\n",
    "    inp_price = Input(shape=(1,))\n",
    "    nsy_price = GaussianNoise(0.1)(inp_price)\n",
    "    \n",
    "    emb_size = 32\n",
    "    emb_reg  = Embedding(len(region_tk)+1, emb_size)(inp_reg)\n",
    "    emb_city = Embedding(len(city_tk)+1, emb_size)(inp_city)\n",
    "    emb_cat1 = Embedding(len(cat1_tk)+1, emb_size)(inp_cat1)\n",
    "    emb_cat2 = Embedding(len(cat2_tk)+1, emb_size)(inp_cat2)\n",
    "    emb_prm1 = Embedding(len(param1_tk)+1, emb_size)(inp_prm1)\n",
    "    emb_prm2 = Embedding(len(param2_tk)+1, emb_size)(inp_prm2)\n",
    "    emb_prm3 = Embedding(len(param3_tk)+1, emb_size)(inp_prm3)\n",
    "    emb_sqnm = Embedding(len(seqnum_tk)+1, emb_size)(inp_sqnm)\n",
    "    emb_usr  = Embedding(len(usertype_tk)+1, emb_size)(inp_usr)\n",
    "    emb_itype= Embedding(len(imgtype_tk)+1, emb_size)(inp_itype)\n",
    "    x = concatenate([emb_reg,emb_city,emb_cat1,emb_cat2,emb_prm1,emb_prm2,emb_prm3,\n",
    "                     emb_sqnm,emb_usr,emb_itype])\n",
    "    x = Flatten()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = concatenate([x, nsy_price])#Do not want to dropout price, its noised up instead. \n",
    "    \n",
    "    x = Dense(512, activation=\"selu\", kernel_initializer=\"lecun_normal\")(x)\n",
    "    x = AlphaDropout(0.1)(x)\n",
    "    x = Dense(256, activation=\"selu\", kernel_initializer=\"lecun_normal\")(x)\n",
    "    x = AlphaDropout(0.1)(x)\n",
    "    x = Dense(128, activation=\"selu\", kernel_initializer=\"lecun_normal\")(x)\n",
    "    x = AlphaDropout(0.1)(x)\n",
    "    x = Dense(64, activation=\"selu\", kernel_initializer=\"lecun_normal\")(x)\n",
    "    x = AlphaDropout(0.05)(x)\n",
    "    x = Dense(32, activation=\"selu\", kernel_initializer=\"lecun_normal\")(x)\n",
    "    x = AlphaDropout(0.05)(x)\n",
    "    x = Dense(8, activation=\"selu\", kernel_initializer=\"lecun_normal\")(x)\n",
    "    y = Dense(1, activation=\"sigmoid\")(x)\n",
    "    \n",
    "    model = Model(inputs=[inp_reg, inp_city, inp_cat1, inp_cat2, inp_prm1, inp_prm2, \n",
    "                          inp_prm3, inp_sqnm, inp_usr, inp_itype, inp_weekday, inp_price],\n",
    "                  outputs=y)\n",
    "    model.compile(optimizer=\"Nadam\", loss=[\"MSE\"], metrics=[root_mean_squared_error])\n",
    "    model.summary()\n",
    "    \n",
    "    return model\n",
    "\n",
    "def root_mean_squared_error(y_true, y_pred):\n",
    "        return K.sqrt(K.mean(K.square(y_pred - y_true), axis=-1)) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ 1,  2,  3, ...,  5,  8, 17]),\n",
       " array([ 1,  2,  3, ...,  5,  9, 26]),\n",
       " array([1, 2, 2, ..., 3, 4, 4]),\n",
       " array([ 1,  2,  3, ..., 15, 17, 17]),\n",
       " array([ 1,  2,  2, ..., 15, 67, 67]),\n",
       " array([ 1,  1,  1, ...,  2, 40, 82]),\n",
       " array([1, 1, 1, ..., 7, 1, 1]),\n",
       " array([   1,    2,    3, ...,   19,   16, 1076]),\n",
       " array([1, 2, 2, ..., 2, 2, 1]),\n",
       " array([  1,   2,   3, ..., 153, 177, 564]),\n",
       " array([5, 1, 0, ..., 6, 4, 3], dtype=int64),\n",
       " array([ 6.2166061 , -1.        ,  7.60140233, ...,  9.10509096,\n",
       "         6.2166061 ,  7.69666708])]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_5 (InputLayer)            (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_6 (InputLayer)            (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_7 (InputLayer)            (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_8 (InputLayer)            (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_9 (InputLayer)            (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_10 (InputLayer)           (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 1, 32)        928         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 1, 32)        55264       input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, 1, 32)        320         input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_4 (Embedding)         (None, 1, 32)        1536        input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_5 (Embedding)         (None, 1, 32)        11904       input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_6 (Embedding)         (None, 1, 32)        8608        input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_7 (Embedding)         (None, 1, 32)        38112       input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_8 (Embedding)         (None, 1, 32)        821824      input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_9 (Embedding)         (None, 1, 32)        128         input_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_10 (Embedding)        (None, 1, 32)        98016       input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 1, 320)       0           embedding_1[0][0]                \n",
      "                                                                 embedding_2[0][0]                \n",
      "                                                                 embedding_3[0][0]                \n",
      "                                                                 embedding_4[0][0]                \n",
      "                                                                 embedding_5[0][0]                \n",
      "                                                                 embedding_6[0][0]                \n",
      "                                                                 embedding_7[0][0]                \n",
      "                                                                 embedding_8[0][0]                \n",
      "                                                                 embedding_9[0][0]                \n",
      "                                                                 embedding_10[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 320)          0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "input_12 (InputLayer)           (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 320)          0           flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "gaussian_noise_1 (GaussianNoise (None, 1)            0           input_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 321)          0           dropout_1[0][0]                  \n",
      "                                                                 gaussian_noise_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 512)          164864      concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "alpha_dropout_1 (AlphaDropout)  (None, 512)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 256)          131328      alpha_dropout_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "alpha_dropout_2 (AlphaDropout)  (None, 256)          0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 128)          32896       alpha_dropout_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "alpha_dropout_3 (AlphaDropout)  (None, 128)          0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 64)           8256        alpha_dropout_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "alpha_dropout_4 (AlphaDropout)  (None, 64)           0           dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 32)           2080        alpha_dropout_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "alpha_dropout_5 (AlphaDropout)  (None, 32)           0           dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 8)            264         alpha_dropout_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 1)            9           dense_6[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 1,376,337\n",
      "Trainable params: 1,376,337\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1277910 samples, validate on 225514 samples\n",
      "Epoch 1/100\n",
      " - 103s - loss: 0.0564 - root_mean_squared_error: 0.1602 - val_loss: 0.0543 - val_root_mean_squared_error: 0.1393\n",
      "Epoch 2/100\n",
      " - 8s - loss: 0.0534 - root_mean_squared_error: 0.1549 - val_loss: 0.0541 - val_root_mean_squared_error: 0.1366\n",
      "Epoch 3/100\n",
      " - 8s - loss: 0.0528 - root_mean_squared_error: 0.1535 - val_loss: 0.0549 - val_root_mean_squared_error: 0.1325\n",
      "Epoch 4/100\n",
      " - 8s - loss: 0.0524 - root_mean_squared_error: 0.1525 - val_loss: 0.0541 - val_root_mean_squared_error: 0.1357\n",
      "Epoch 5/100\n",
      " - 8s - loss: 0.0521 - root_mean_squared_error: 0.1519 - val_loss: 0.0534 - val_root_mean_squared_error: 0.1365\n",
      "Epoch 6/100\n",
      " - 8s - loss: 0.0518 - root_mean_squared_error: 0.1512 - val_loss: 0.0535 - val_root_mean_squared_error: 0.1338\n",
      "Epoch 7/100\n",
      " - 8s - loss: 0.0516 - root_mean_squared_error: 0.1507 - val_loss: 0.0548 - val_root_mean_squared_error: 0.1303\n",
      "Epoch 8/100\n",
      " - 8s - loss: 0.0514 - root_mean_squared_error: 0.1503 - val_loss: 0.0539 - val_root_mean_squared_error: 0.1326\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "Epoch 9/100\n",
      " - 8s - loss: 0.0508 - root_mean_squared_error: 0.1492 - val_loss: 0.0538 - val_root_mean_squared_error: 0.1327\n",
      "Epoch 10/100\n",
      " - 8s - loss: 0.0507 - root_mean_squared_error: 0.1490 - val_loss: 0.0536 - val_root_mean_squared_error: 0.1333\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 2.0000000949949027e-05.\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'input//sample_submission.csv' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-464af2764c66>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'model_baseline_weights.hdf5'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mpreds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0msubmission\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\"/sample_submission.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[0msubmission\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'deal_probability'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreds\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[0msubmission\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"submission.csv\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    707\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[0;32m    708\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 709\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    710\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    711\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    447\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    448\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 449\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    450\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    451\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    816\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    817\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 818\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    819\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    820\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1047\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'c'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1048\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'c'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1049\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1050\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1051\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'python'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1693\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'allow_leading_cols'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1694\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1695\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1696\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1697\u001b[0m         \u001b[1;31m# XXX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: File b'input//sample_submission.csv' does not exist"
     ]
    }
   ],
   "source": [
    "model = build_model()\n",
    "\n",
    "earlystop = EarlyStopping(monitor=\"val_loss\",mode=\"auto\",patience=5,verbose=0)\n",
    "checkpt = ModelCheckpoint(monitor=\"val_loss\",mode=\"auto\",filepath='model_baseline_weights.hdf5',verbose=0,save_best_only=True)\n",
    "rlrop = ReduceLROnPlateau(monitor='val_loss',mode='auto',patience=2,verbose=1,factor=0.1,cooldown=0,min_lr=1e-6)\n",
    "batch_size = 2048\n",
    "model.fit(x_train, y_train,batch_size=batch_size,validation_data=(x_val, y_val),\n",
    "          epochs=100,verbose=2,callbacks =[checkpt, earlystop, rlrop])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('model_baseline_weights.hdf5')\n",
    "preds = model.predict(x_test, batch_size=batch_size)\n",
    "submission = pd.read_csv(data_dir+\"/sample_submission.csv\")\n",
    "submission['deal_probability'] = preds\n",
    "submission.to_csv(\"submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

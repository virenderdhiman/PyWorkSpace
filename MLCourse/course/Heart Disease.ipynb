{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Heart Disease\n",
    "\n",
    "This dataset contains 76 features, but all published experiments refer to using a subset of 14 of them. The \"goal\" feature refers to the presence of heart disease in the patient. It is integer valued from 0 (no presence) to 4 (values 1,2,3,4) from absence (value 0). It is therefore a multiclass classification problem.\n",
    "\n",
    "*For our example, we will use several more features than the traditional 14.*\n",
    "\n",
    "Feature info (attributes used): \n",
    "1. feature 3 (age) - Age in years\n",
    "2. feature 4 (sex) - male or female\n",
    "3. feature 9 (cp) - chest pain type (typical angina, atypical angina, non-anginal, asymptomatic)\n",
    "4. feature 10 (trestbps) - resting blood pressure (mm Hg)\n",
    "5. feature 12 (chol) - cholesterol (mg/dl)\n",
    "6. feature 14 (cigperday) - number of cigarettes per day\n",
    "7. feature 16 (fbs) - fasting blood sugar > 120 mg/dl (1 = true; 0 = false) \n",
    "8. feature 18 (famhist) - family history of heart disease (1 = true; 0 = false)\n",
    "9. feature 19 (restecg) - resting electrocardiographic results (normal; st-t = having ST-T wave abnormality (T wave inversions and/or ST elevation or depression of > 0.05 mV); vent = showing probable or definite left ventricular hypertrophy by Estes' criteria)\n",
    "10. feature 32 (thalach) - maximum heart rate achieved\n",
    "11. feature 38 (exang) - exercise induced angina (1 = yes; 0 = no)\n",
    "12. feature 40 (oldpeak) - ST depression induced by exercise relative to rest\n",
    "13. feature 41 (slope) - the slope of the peak exercise ST segment (upsloping, flat, downsloping)\n",
    "14. feature 44 (ca) - number of major vessels (0-3) colored by flourosopy\n",
    "15. feature 51 (thal) - normal, fixed defect, or reversable defect\n",
    "16. feature 58 (target) (the predicted attribute) \n",
    "  - 0: < 50% diameter narrowing\n",
    "  - 1+: > 50% diameter narrowing\n",
    "\n",
    "### Our focus in using this dataset will be exploring pre-processing methods more thoroughly\n",
    "\n",
    "More details can be found at [the UCI repository](https://archive.ics.uci.edu/ml/datasets/Heart+Disease).\n",
    "\n",
    "### Acknowledgments\n",
    "\n",
    "The authors of the dataset have requested that any use of the data include the names of the principal investigator responsible for the data collection at each institution. They would be: \n",
    "\n",
    "1. Hungarian Institute of Cardiology. Budapest: Andras Janosi, M.D. \n",
    "2. University Hospital, Zurich, Switzerland: William Steinbrunn, M.D. \n",
    "3. University Hospital, Basel, Switzerland: Matthias Pfisterer, M.D. \n",
    "4. V.A. Medical Center, Long Beach and Cleveland Clinic Foundation:Robert Detrano, M.D., Ph.D."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data from CSV\n",
    "\n",
    "We can read the data directly from the CSV located in the [data/](data/) directory. The [raw data](data/heart-disease-raw.csv) was pre-processed to re-name categorical features where they are otherwise ordinal variables. This allows us to walk through an entire pre-processing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>cigperday</th>\n",
       "      <th>fbs</th>\n",
       "      <th>famhist</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>63</td>\n",
       "      <td>male</td>\n",
       "      <td>typical anginal</td>\n",
       "      <td>145</td>\n",
       "      <td>233</td>\n",
       "      <td>50.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>vent</td>\n",
       "      <td>150</td>\n",
       "      <td>0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>downsloping</td>\n",
       "      <td>0.0</td>\n",
       "      <td>fixed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>67</td>\n",
       "      <td>male</td>\n",
       "      <td>asymptomatic</td>\n",
       "      <td>160</td>\n",
       "      <td>286</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>vent</td>\n",
       "      <td>108</td>\n",
       "      <td>1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>flat</td>\n",
       "      <td>3.0</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>67</td>\n",
       "      <td>male</td>\n",
       "      <td>asymptomatic</td>\n",
       "      <td>120</td>\n",
       "      <td>229</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>vent</td>\n",
       "      <td>129</td>\n",
       "      <td>1</td>\n",
       "      <td>2.6</td>\n",
       "      <td>flat</td>\n",
       "      <td>2.0</td>\n",
       "      <td>reversable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>37</td>\n",
       "      <td>male</td>\n",
       "      <td>non-anginal</td>\n",
       "      <td>130</td>\n",
       "      <td>250</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>normal</td>\n",
       "      <td>187</td>\n",
       "      <td>0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>downsloping</td>\n",
       "      <td>0.0</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>41</td>\n",
       "      <td>female</td>\n",
       "      <td>atypical anginal</td>\n",
       "      <td>130</td>\n",
       "      <td>204</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>vent</td>\n",
       "      <td>172</td>\n",
       "      <td>0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>upsloping</td>\n",
       "      <td>0.0</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age     sex                cp  trestbps  chol  cigperday  fbs  famhist  \\\n",
       "0   63    male   typical anginal       145   233       50.0    1        1   \n",
       "1   67    male      asymptomatic       160   286       40.0    0        1   \n",
       "2   67    male      asymptomatic       120   229       20.0    0        1   \n",
       "3   37    male       non-anginal       130   250        0.0    0        1   \n",
       "4   41  female  atypical anginal       130   204        0.0    0        1   \n",
       "\n",
       "  restecg  thalach  exang  oldpeak        slope   ca        thal  \n",
       "0    vent      150      0      2.3  downsloping  0.0       fixed  \n",
       "1    vent      108      1      1.5         flat  3.0      normal  \n",
       "2    vent      129      1      2.6         flat  2.0  reversable  \n",
       "3  normal      187      0      3.5  downsloping  0.0      normal  \n",
       "4    vent      172      0      1.4    upsloping  0.0      normal  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# read the raw csv\n",
    "X = pd.read_csv('data/heart-disease-2.csv', header=None)\n",
    "\n",
    "# rename the columns\n",
    "cols = ['age', 'sex', 'cp', 'trestbps', 'chol', 'cigperday', 'fbs', 'famhist',\n",
    "        'restecg', 'thalach', 'exang', 'oldpeak', 'slope', 'ca', 'thal', 'target']\n",
    "\n",
    "X.columns = cols\n",
    "y = X.pop('target')  # don't want target in the X matrix\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-split: any major imbalance?\n",
    "\n",
    "If there are any categorical features with rare factor levels that need to be considered before splitting, we'll find out here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sex\n",
      "male      206\n",
      "female     97\n",
      "Name: sex, dtype: int64\n",
      "\n",
      "cp\n",
      "asymptomatic        144\n",
      "non-anginal          86\n",
      "atypical anginal     50\n",
      "typical anginal      23\n",
      "Name: cp, dtype: int64\n",
      "\n",
      "restecg\n",
      "normal    151\n",
      "vent      148\n",
      "st-t        4\n",
      "Name: restecg, dtype: int64\n",
      "\n",
      "slope\n",
      "upsloping      142\n",
      "flat           140\n",
      "downsloping     21\n",
      "Name: slope, dtype: int64\n",
      "\n",
      "thal\n",
      "normal        166\n",
      "reversable    117\n",
      "fixed          18\n",
      "Name: thal, dtype: int64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def examine_cats(frame):\n",
    "    for catcol in frame.columns[frame.dtypes == 'object'].tolist():\n",
    "        print(catcol)\n",
    "        print(frame[catcol].value_counts())\n",
    "        print(\"\")\n",
    "        \n",
    "examine_cats(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform train/test split\n",
    "\n",
    "Remember, we always need to split! We will also stratify on the '`restecg`' variable since it's the most likely to be poorly split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 227\n",
      "Test size: 76\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>cigperday</th>\n",
       "      <th>fbs</th>\n",
       "      <th>famhist</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>54</td>\n",
       "      <td>female</td>\n",
       "      <td>atypical anginal</td>\n",
       "      <td>132</td>\n",
       "      <td>288</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>vent</td>\n",
       "      <td>159</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>upsloping</td>\n",
       "      <td>1.0</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>52</td>\n",
       "      <td>male</td>\n",
       "      <td>non-anginal</td>\n",
       "      <td>138</td>\n",
       "      <td>223</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>normal</td>\n",
       "      <td>169</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>upsloping</td>\n",
       "      <td>NaN</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>57</td>\n",
       "      <td>male</td>\n",
       "      <td>asymptomatic</td>\n",
       "      <td>130</td>\n",
       "      <td>131</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>normal</td>\n",
       "      <td>115</td>\n",
       "      <td>1</td>\n",
       "      <td>1.2</td>\n",
       "      <td>flat</td>\n",
       "      <td>1.0</td>\n",
       "      <td>reversable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>63</td>\n",
       "      <td>female</td>\n",
       "      <td>atypical anginal</td>\n",
       "      <td>140</td>\n",
       "      <td>195</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>normal</td>\n",
       "      <td>179</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>upsloping</td>\n",
       "      <td>2.0</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>52</td>\n",
       "      <td>male</td>\n",
       "      <td>atypical anginal</td>\n",
       "      <td>120</td>\n",
       "      <td>325</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>normal</td>\n",
       "      <td>172</td>\n",
       "      <td>0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>upsloping</td>\n",
       "      <td>0.0</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     age     sex                cp  trestbps  chol  cigperday  fbs  famhist  \\\n",
       "167   54  female  atypical anginal       132   288        0.0    1        0   \n",
       "166   52    male       non-anginal       138   223       50.0    0        1   \n",
       "300   57    male      asymptomatic       130   131       50.0    0        0   \n",
       "185   63  female  atypical anginal       140   195        2.0    0        1   \n",
       "84    52    male  atypical anginal       120   325       30.0    0        1   \n",
       "\n",
       "    restecg  thalach  exang  oldpeak      slope   ca        thal  \n",
       "167    vent      159      1      0.0  upsloping  1.0      normal  \n",
       "166  normal      169      0      0.0  upsloping  NaN      normal  \n",
       "300  normal      115      1      1.2       flat  1.0  reversable  \n",
       "185  normal      179      0      0.0  upsloping  2.0      normal  \n",
       "84   normal      172      0      0.2  upsloping  0.0      normal  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "seed = 42\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=seed, \n",
    "                                                    stratify=X['restecg'])\n",
    "\n",
    "print(\"Train size: %i\" % X_train.shape[0])\n",
    "print(\"Test size: %i\" % X_test.shape[0])\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sex\n",
      "male      153\n",
      "female     74\n",
      "Name: sex, dtype: int64\n",
      "\n",
      "cp\n",
      "asymptomatic        105\n",
      "non-anginal          66\n",
      "atypical anginal     37\n",
      "typical anginal      19\n",
      "Name: cp, dtype: int64\n",
      "\n",
      "restecg\n",
      "normal    113\n",
      "vent      111\n",
      "st-t        3\n",
      "Name: restecg, dtype: int64\n",
      "\n",
      "slope\n",
      "flat           110\n",
      "upsloping      102\n",
      "downsloping     15\n",
      "Name: slope, dtype: int64\n",
      "\n",
      "thal\n",
      "normal        121\n",
      "reversable     92\n",
      "fixed          12\n",
      "Name: thal, dtype: int64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "examine_cats(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Transformers\n",
    "\n",
    "There are several custom transformers that will be useful for this data:\n",
    "\n",
    "- Custom one-hot encoding that drops one level to avoid the [dummy variable trap](http://www.algosome.com/articles/dummy-variable-trap-regression.html)\n",
    "- Model-based imputation of continuous variables, since mean/median centering is rudimentary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom base class\n",
    "\n",
    "We'll start with a cusom base class that depends on the input to be a Pandas dataframe. This base class will provide super methods for validating the input type as well as the presence of any prescribed columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.utils.validation import check_is_fitted\n",
    "\n",
    "class CustomPandasTransformer(BaseEstimator, TransformerMixin):\n",
    "    def _validate_input(self, X):\n",
    "        if not isinstance(X, pd.DataFrame):\n",
    "            raise TypeError(\"X must be a DataFrame, but got type=%s\" \n",
    "                            % type(X))\n",
    "        return X\n",
    "    \n",
    "    @staticmethod\n",
    "    def _validate_columns(X, cols):\n",
    "        scols = set(X.columns)  # set for O(1) lookup\n",
    "        if not all(c in scols for c in cols):\n",
    "            raise ValueError(\"all columns must be present in X\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One-hot encode categorical data\n",
    "\n",
    "It is probably (hopefully) obvious why we need to handle data that is in string format. There is not much we can do numerically with data that resembles the following:\n",
    "\n",
    "    [flat, upsloping, downsloping, ..., flat, flat, downsloping]\n",
    "    \n",
    "There is a natural procedure to force numericism amongst string data: map each unique string to a unique level (1, 2, 3). This is, in fact, exactly what the sklearn [`LabelEncoder`](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html) does. However, this is not sufficient for modeling purposes, since most algorithms will treat this as [ordinal data](https://en.wikipedia.org/wiki/Ordinal_data), where in many cases it is not. Imagine you fit a regression on data you've label-encoded, and one feature (type of chest pain, for instance) is now:\n",
    "\n",
    "    [0, 2, 3, ..., 1, 0]\n",
    "    \n",
    "You might get coefficients back that make no sense since \"asymptomatic\" or \"non-anginal\", etc., are not inherently numerically greater or less than one another. Therefore, we [*one-hot encode*](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html) our categorical data into a numerical representation. Now we have dummy data and a binary feature for each variable/factor-level combination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "\n",
    "class DummyEncoder(CustomPandasTransformer):\n",
    "    def __init__(self, columns, sep='_', drop_one_level=True, tmp_nan_rep='N/A'):\n",
    "        self.columns = columns\n",
    "        self.sep = sep\n",
    "        self.drop_one_level = drop_one_level\n",
    "        self.tmp_nan_rep = tmp_nan_rep\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        # validate the input\n",
    "        X = self._validate_input(X).copy()  # get a copy\n",
    "        \n",
    "        # parameter validation happens here:\n",
    "        tmp_nan = self.tmp_nan_rep\n",
    "        \n",
    "        # validate all the columns present\n",
    "        cols = self.columns\n",
    "        self._validate_columns(X, cols)\n",
    "                \n",
    "        # for each column, fit a label encoder\n",
    "        lab_encoders = {}\n",
    "        for col in cols:\n",
    "            vec = [tmp_nan if pd.isnull(v) \n",
    "                   else v for v in X[col].tolist()]\n",
    "            \n",
    "            # if the tmp_nan value is not present in vec, make sure it is\n",
    "            # so the transform won't break down\n",
    "            svec = list(set(vec))\n",
    "            if tmp_nan not in svec:\n",
    "                svec.append(tmp_nan)\n",
    "            \n",
    "            le = LabelEncoder()\n",
    "            lab_encoders[col] = le.fit(svec)\n",
    "            \n",
    "            # transform the column, re-assign\n",
    "            X[col] = le.transform(vec)\n",
    "            \n",
    "        # fit a single OHE on the transformed columns - but we need to ensure\n",
    "        # the N/A tmp_nan vals make it into the OHE or it will break down later.\n",
    "        # this is a hack - add a row of all transformed nan levels\n",
    "        ohe_set = X[cols]\n",
    "        ohe_nan_row = {c: lab_encoders[c].transform([tmp_nan])[0] for c in cols}\n",
    "        ohe_set = ohe_set.append(ohe_nan_row, ignore_index=True)\n",
    "        ohe = OneHotEncoder(sparse=False).fit(ohe_set)\n",
    "        \n",
    "        # assign fit params\n",
    "        self.ohe_ = ohe\n",
    "        self.le_ = lab_encoders\n",
    "        self.cols_ = cols\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        check_is_fitted(self, 'ohe_')\n",
    "        X = self._validate_input(X).copy()\n",
    "        \n",
    "        # fit params that we need\n",
    "        ohe = self.ohe_\n",
    "        lenc = self.le_\n",
    "        cols = self.cols_\n",
    "        tmp_nan = self.tmp_nan_rep\n",
    "        sep = self.sep\n",
    "        drop = self.drop_one_level\n",
    "        \n",
    "        # validate the cols and the new X\n",
    "        self._validate_columns(X, cols)\n",
    "        col_order = []\n",
    "        drops = []\n",
    "        \n",
    "        for col in cols:\n",
    "            # get the vec from X, transform its nans if present\n",
    "            vec = [tmp_nan if pd.isnull(v) \n",
    "                   else v for v in X[col].tolist()]\n",
    "            \n",
    "            le = lenc[col]\n",
    "            vec_trans = le.transform(vec)  # str -> int\n",
    "            X[col] = vec_trans\n",
    "            \n",
    "            # get the column names (levels) so we can predict the \n",
    "            # order of the output cols\n",
    "            classes = [\"%s%s%s\" % (col, sep, clz) for clz in le.classes_.tolist()]\n",
    "            col_order.extend(classes)\n",
    "            \n",
    "            # if we want to drop one, just drop the last\n",
    "            if drop:\n",
    "                drops.append(classes[-1])\n",
    "                \n",
    "        # now we can get the transformed OHE\n",
    "        ohe_trans = pd.DataFrame.from_records(data=ohe.transform(X[cols]), \n",
    "                                              columns=col_order)\n",
    "        \n",
    "        # set the index to be equal to X's for a smooth concat\n",
    "        ohe_trans.index = X.index\n",
    "        \n",
    "        # if we're dropping one level, do so now\n",
    "        if drops:\n",
    "            ohe_trans = ohe_trans.drop(drops, axis=1)\n",
    "        \n",
    "        # drop the original columns from X\n",
    "        X = X.drop(cols, axis=1)\n",
    "        \n",
    "        # concat the new columns\n",
    "        X = pd.concat([X, ohe_trans], axis=1)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>cigperday</th>\n",
       "      <th>fbs</th>\n",
       "      <th>famhist</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>ca</th>\n",
       "      <th>...</th>\n",
       "      <th>cp_non-anginal</th>\n",
       "      <th>restecg_N/A</th>\n",
       "      <th>restecg_normal</th>\n",
       "      <th>restecg_st-t</th>\n",
       "      <th>slope_N/A</th>\n",
       "      <th>slope_downsloping</th>\n",
       "      <th>slope_flat</th>\n",
       "      <th>thal_N/A</th>\n",
       "      <th>thal_fixed</th>\n",
       "      <th>thal_normal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>54</td>\n",
       "      <td>132</td>\n",
       "      <td>288</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>159</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>52</td>\n",
       "      <td>138</td>\n",
       "      <td>223</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>169</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>57</td>\n",
       "      <td>130</td>\n",
       "      <td>131</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>115</td>\n",
       "      <td>1</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>63</td>\n",
       "      <td>140</td>\n",
       "      <td>195</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>179</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>52</td>\n",
       "      <td>120</td>\n",
       "      <td>325</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>172</td>\n",
       "      <td>0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     age  trestbps  chol  cigperday  fbs  famhist  thalach  exang  oldpeak  \\\n",
       "167   54       132   288        0.0    1        0      159      1      0.0   \n",
       "166   52       138   223       50.0    0        1      169      0      0.0   \n",
       "300   57       130   131       50.0    0        0      115      1      1.2   \n",
       "185   63       140   195        2.0    0        1      179      0      0.0   \n",
       "84    52       120   325       30.0    0        1      172      0      0.2   \n",
       "\n",
       "      ca     ...       cp_non-anginal  restecg_N/A  restecg_normal  \\\n",
       "167  1.0     ...                  0.0          0.0             0.0   \n",
       "166  NaN     ...                  1.0          0.0             1.0   \n",
       "300  1.0     ...                  0.0          0.0             1.0   \n",
       "185  2.0     ...                  0.0          0.0             1.0   \n",
       "84   0.0     ...                  0.0          0.0             1.0   \n",
       "\n",
       "     restecg_st-t  slope_N/A  slope_downsloping  slope_flat  thal_N/A  \\\n",
       "167           0.0        0.0                0.0         0.0       0.0   \n",
       "166           0.0        0.0                0.0         0.0       0.0   \n",
       "300           0.0        0.0                0.0         1.0       0.0   \n",
       "185           0.0        0.0                0.0         0.0       0.0   \n",
       "84            0.0        0.0                0.0         0.0       0.0   \n",
       "\n",
       "     thal_fixed  thal_normal  \n",
       "167         0.0          1.0  \n",
       "166         0.0          1.0  \n",
       "300         0.0          0.0  \n",
       "185         0.0          1.0  \n",
       "84          0.0          1.0  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "de = DummyEncoder(columns=['sex', 'cp', 'restecg', 'slope', 'thal'])\n",
    "de.fit(X_train)\n",
    "X_train_dummied = de.transform(X_train)\n",
    "X_train_dummied.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model-based imputation\n",
    "\n",
    "As discussed in the iris notebook, there are many pitfalls to using mean or median for scaling. In instances where our data is too large to examine all features graphically, many times we cannot discern whether all features are normally distributed (a pre-requisite for mean-scaling). If we want to get more sophisticated, we can use an approach for imputation that is based on a model; we will use a [`BaggingRegressor`](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingRegressor.html) (since we are filling in NaN continuous variables only at this point).\n",
    "\n",
    "Note that there are other common approaches for this, like KNN imputation, but nearest neighbors models require your data to be scaled, which we're trying to avoid.\n",
    "\n",
    "### Beware:\n",
    "\n",
    "Sometimes missing data is informative. For instance... failure to report `cigperday` could be a bias on part of the patient who may not want to receive judgment or a lecture, or could indicate 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.externals import six\n",
    "\n",
    "class BaggedRegressorImputer(CustomPandasTransformer):\n",
    "    def __init__(self, impute_cols, base_estimator=None, n_estimators=10, \n",
    "                 max_samples=1.0, max_features=1.0, bootstrap=True, \n",
    "                 bootstrap_features=False, n_jobs=1,\n",
    "                 random_state=None, verbose=0):\n",
    "        \n",
    "        self.impute_cols = impute_cols\n",
    "        self.base_estimator = base_estimator\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_samples = max_samples\n",
    "        self.max_features = max_features\n",
    "        self.bootstrap = bootstrap\n",
    "        self.bootstrap_features = bootstrap_features\n",
    "        self.n_jobs = n_jobs\n",
    "        self.random_state = random_state\n",
    "        self.verbose = verbose\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        X = self._validate_input(X)  # don't need a copy this time\n",
    "        \n",
    "        # validate the columns\n",
    "        cols = self.impute_cols\n",
    "        self._validate_columns(X, cols)\n",
    "        \n",
    "        # drop off the columns we'll be imputing as targets\n",
    "        regressors = {}\n",
    "        targets = {c: X[c] for c in cols}\n",
    "        X = X.drop(cols, axis=1)  # these should all be filled in (no NaN)\n",
    "        \n",
    "        for k, target in six.iteritems(targets):\n",
    "            # split X row-wise into train/test where test is the missing\n",
    "            # rows in the target\n",
    "            test_mask = pd.isnull(target)\n",
    "            train = X.loc[~test_mask]\n",
    "            train_y = target[~test_mask]\n",
    "            \n",
    "            # fit the regressor\n",
    "            regressors[k] = BaggingRegressor(\n",
    "                base_estimator=self.base_estimator,\n",
    "                n_estimators=self.n_estimators,\n",
    "                max_samples=self.max_samples,\n",
    "                max_features=self.max_features,\n",
    "                bootstrap=self.bootstrap,\n",
    "                bootstrap_features=self.bootstrap_features,\n",
    "                n_jobs=self.n_jobs, \n",
    "                random_state=self.random_state,\n",
    "                verbose=self.verbose, oob_score=False,\n",
    "                warm_start=False).fit(train, train_y)\n",
    "            \n",
    "        # assign fit params\n",
    "        self.regressors_ = regressors\n",
    "        return self\n",
    "        \n",
    "    def transform(self, X):\n",
    "        check_is_fitted(self, 'regressors_')\n",
    "        X = self._validate_input(X).copy()  # need a copy\n",
    "        \n",
    "        cols = self.impute_cols\n",
    "        self._validate_columns(X, cols)\n",
    "        \n",
    "        # fill in the missing\n",
    "        models = self.regressors_\n",
    "        for k, model in six.iteritems(models):\n",
    "            target = X[k]\n",
    "            \n",
    "            # split X row-wise into train/test where test is the missing\n",
    "            # rows in the target\n",
    "            test_mask = pd.isnull(target)\n",
    "            \n",
    "            # if there's nothing missing in the test set for this feature, skip\n",
    "            if test_mask.sum() == 0:\n",
    "                continue\n",
    "            test = X.loc[test_mask].drop(cols, axis=1)  # drop impute cols\n",
    "            \n",
    "            # generate predictions\n",
    "            preds = model.predict(test)\n",
    "            \n",
    "            # impute!\n",
    "            X.loc[test_mask, k] = preds\n",
    "            \n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bagged_imputer = BaggedRegressorImputer(impute_cols=['cigperday', 'ca'], \n",
    "                                        random_state=seed)\n",
    "bagged_imputer.fit(X_train_dummied)\n",
    "\n",
    "# save the masks so we can look at them afterwards\n",
    "ca_nan_mask = pd.isnull(X_train_dummied.ca)\n",
    "cpd_nan_mask = pd.isnull(X_train_dummied.cigperday)\n",
    "\n",
    "# impute\n",
    "X_train_imputed = bagged_imputer.transform(X_train_dummied)\n",
    "X_train_imputed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_imputed[ca_nan_mask].ca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_imputed[cpd_nan_mask].cigperday"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature selection/dimensionality reduction\n",
    "\n",
    "Often times, when there is very high-dimensional data (100s or 1000s of features), it's useful to perform feature selection techniques to create more simple models that can be understood by analysts. A common one is [principal components analysis](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html), but one of its drawbacks is diminished model clarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train_imputed)\n",
    "\n",
    "# fit PCA, get explained variance of ALL features\n",
    "pca_all = PCA(n_components=None)\n",
    "pca_all.fit(scaler.transform(X_train_imputed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explained_var = np.cumsum(pca_all.explained_variance_ratio_)\n",
    "explained_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "x_axis = np.arange(X_train_imputed.shape[1]) + 1\n",
    "plt.plot(x_axis, explained_var)\n",
    "\n",
    "# At which point to cut off?\n",
    "minexp = np.where(explained_var > 0.9)[0][0]\n",
    "plt.axvline(x=minexp, linestyle='dashed', color='red', alpha=0.5)\n",
    "plt.xticks(x_axis)\n",
    "plt.show()\n",
    "\n",
    "print(\"Cumulative explained variance at %i components: %.5f\" % (minexp, explained_var[minexp]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At 15 (of 25) features, we finally explain >90% cumulative variance in our components. This is not a significant enough feature reduction to warrant use of PCA, so we'll skip it.\n",
    "\n",
    "# Baseline several models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# set up our CV\n",
    "cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=seed)\n",
    "stages = [\n",
    "    ('dummy', DummyEncoder(columns=['sex', 'cp', 'restecg', 'slope', 'thal'])),\n",
    "    ('impute', BaggedRegressorImputer(impute_cols=['cigperday', 'ca'], random_state=seed))\n",
    "]\n",
    "\n",
    "def build_pipeline(pipe_stages, estimator, est_name='clf'):\n",
    "    # copy the stages\n",
    "    pipe_stages = [stage for stage in pipe_stages]\n",
    "    pipe_stages.append((est_name, estimator))\n",
    "    \n",
    "    # return the pipe\n",
    "    return Pipeline(pipe_stages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# fit a Logistic regression\n",
    "lgr_pipe = build_pipeline(stages, LogisticRegression(random_state=seed))\n",
    "cross_val_score(lgr_pipe, X=X_train, y=y_train, scoring='neg_log_loss', cv=cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# fit a GBM\n",
    "gbm_pipe = build_pipeline(stages, GradientBoostingClassifier(n_estimators=25, max_depth=3, random_state=seed))\n",
    "cross_val_score(gbm_pipe, X=X_train, y=y_train, scoring='neg_log_loss', cv=cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# fit a RF\n",
    "rf_pipe = build_pipeline(stages, RandomForestClassifier(n_estimators=25, random_state=seed))\n",
    "cross_val_score(rf_pipe, X=X_train, y=y_train, scoring='neg_log_loss', cv=cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning hyper-params\n",
    "\n",
    "Now that we've baselined several models, let's choose a couple of the better-performing models to tune."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import randint, uniform\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "gbm_pipe = Pipeline([\n",
    "    ('dummy', DummyEncoder(columns=['sex', 'cp', 'restecg', 'slope', 'thal'])),\n",
    "    ('impute', BaggedRegressorImputer(impute_cols=['cigperday', 'ca'], random_state=seed)),\n",
    "    ('clf', GradientBoostingClassifier(random_state=seed))\n",
    "])\n",
    "\n",
    "# define the hyper-params\n",
    "hyper_params = {\n",
    "    'impute__n_estimators': randint(10, 50),\n",
    "    'impute__max_samples': uniform(0.75, 0.125),\n",
    "    'impute__max_features': uniform(0.75, 0.125),\n",
    "    'clf__n_estimators': randint(50, 400),\n",
    "    'clf__max_depth': [1, 3, 4, 5, 7],\n",
    "    'clf__learning_rate': uniform(0.05, 0.1),\n",
    "    'clf__min_samples_split': [2, 4, 5, 10],\n",
    "    'clf__min_samples_leaf': [1, 2, 5]\n",
    "}\n",
    "\n",
    "# define the search\n",
    "gbm_search = RandomizedSearchCV(gbm_pipe, param_distributions=hyper_params,\n",
    "                                random_state=seed, cv=cv, n_iter=100,\n",
    "                                n_jobs=-1, verbose=1, scoring='neg_log_loss',\n",
    "                                return_train_score=False)\n",
    "\n",
    "gbm_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgr_pipe = Pipeline([\n",
    "    ('dummy', DummyEncoder(columns=['sex', 'cp', 'restecg', 'slope', 'thal'])),\n",
    "    ('impute', BaggedRegressorImputer(impute_cols=['cigperday', 'ca'], random_state=seed)),\n",
    "    ('clf', LogisticRegression(random_state=seed))\n",
    "])\n",
    "\n",
    "# define the hyper-params\n",
    "hyper_params = {\n",
    "    'impute__n_estimators': randint(10, 50),\n",
    "    'impute__max_samples': uniform(0.75, 0.125),\n",
    "    'impute__max_features': uniform(0.75, 0.125),\n",
    "    'clf__penalty': ['l1', 'l2'],\n",
    "    'clf__C': uniform(0.5, 0.125),\n",
    "    'clf__max_iter': randint(100, 500)\n",
    "}\n",
    "\n",
    "# define the search\n",
    "lgr_search = RandomizedSearchCV(lgr_pipe, param_distributions=hyper_params,\n",
    "                                random_state=seed, cv=cv, n_iter=100,\n",
    "                                n_jobs=-1, verbose=1, scoring='neg_log_loss',\n",
    "                                return_train_score=False)\n",
    "\n",
    "lgr_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examine the results\n",
    "\n",
    "Right away we can tell that the logistic regression model was *much* faster than the gradient boosting model. However, does the extra time spent fitting end up giving us a performance boost? Let's introduce our test set to the optimized models and select the one that performs better. We are using [__log loss__](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.log_loss.html) as a scoring metric.\n",
    "\n",
    "See [this answer](https://stats.stackexchange.com/questions/208443/intuitive-explanation-of-logloss) for a full intuitive explanation of log loss, but note that lower (closer to zero) is better. There is no maximum to log loss, and typically, the more classes you have, the higher it will be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "\n",
    "gbm_preds = gbm_search.predict_proba(X_test)\n",
    "lgr_preds = lgr_search.predict_proba(X_test)\n",
    "\n",
    "print(\"GBM test LOSS: %.5f\" % log_loss(y_true=y_test, y_pred=gbm_preds))\n",
    "print(\"Logistic regression test LOSS: %.5f\" % log_loss(y_true=y_test, y_pred=lgr_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Note that in log loss, greater is WORSE. Therefore, the logistic regression was out-performed by the GBM. If the greater time to fit is not an issue for you, then this would be the better model to select. Likewise, you may favor model transparency over the extra few decimal points of accuracy, in which case the logistic regression might be favorable.\n",
    "\n",
    "# Variable importance\n",
    "\n",
    "Most times, it's not enough to build a good model. Most executives will want to know *why* something works. Moreover, in regulated industries like banking or insurance, knowing why a model is working is incredibly important for defending models to a regulatory board.\n",
    "\n",
    "One of the methods commonly used for observing variable importance for non-linear methods (like our gradient boosting model) is to break the model into piecewise linear functions and measure how the model performs against each variable. This is called a \"partial dependency plot.\"\n",
    "\n",
    "### Raw feature importances\n",
    "\n",
    "We can get the raw feature importances from the estimator itself, and match them up with the transformed column names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feed data through the pipe stages to get the transformed feature names\n",
    "X_trans = X_train\n",
    "for step in gbm_search.best_estimator_.steps[:-1]:\n",
    "    X_trans = step[1].transform(X_trans)\n",
    "    \n",
    "transformed_feature_names = X_trans.columns\n",
    "transformed_feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_gbm = gbm_search.best_estimator_.steps[-1][1]\n",
    "importances = best_gbm.feature_importances_\n",
    "importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances = sorted(zip(np.arange(len(transformed_feature_names)), \n",
    "                                 transformed_feature_names, \n",
    "                                 importances), \n",
    "                             key=(lambda ici: ici[2]),\n",
    "                             reverse=True)\n",
    "\n",
    "feature_importances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partial dependency\n",
    "\n",
    "In the following section, we'll break our GBM into a piecewise linear functions to gauge how different variables impact the target, and create [partial dependency plots](http://scikit-learn.org/stable/auto_examples/ensemble/plot_partial_dependence.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble.partial_dependence import plot_partial_dependence\n",
    "from sklearn.ensemble.partial_dependence import partial_dependence\n",
    "\n",
    "def plot_partial(est, which_features, X, names, label):\n",
    "    fig, axs = plot_partial_dependence(est, X, which_features,\n",
    "                                       feature_names=names,\n",
    "                                       n_jobs=3, grid_resolution=50,\n",
    "                                       label=label)\n",
    "    \n",
    "    fig.suptitle('Partial dependence of %i features\\n'\n",
    "                 'on heart disease' % (len(which_features)))\n",
    "    plt.subplots_adjust(top=0.8)  # tight_layout causes overlap with suptitle\n",
    "        \n",
    "plot_partial(est=best_gbm, X=X_trans,\n",
    "             which_features=[2, 8, 9, 0, 6, (2, 9)],\n",
    "             names=transformed_feature_names,\n",
    "             label=1)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

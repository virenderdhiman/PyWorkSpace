{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dhiman\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This model is designed to use embeddings to deal with the categorical variables, while ignoring \n",
    "some of the data like the periods, images, titles and descriptions. \n",
    "Most of the remaining data is categorical, except for the price. \n",
    "A simple FFNN model seems to do pretty good work. \n",
    "\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input, Embedding, Dropout, Flatten\n",
    "from keras.layers.merge import concatenate, dot, multiply, add\n",
    "from keras.callbacks import Callback, EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras.optimizers import Nadam, RMSprop, adam\n",
    "from keras.layers.noise import AlphaDropout, GaussianNoise\n",
    "from keras import backend as K\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "##================Import Data, Replace NaNs with -1\n",
    "data_dir = \"input/\"\n",
    "train_data = pd.read_csv(data_dir+\"/train.csv\", parse_dates=[\"activation_date\"]) #we will eventually turn the date column into day of week [0,6]\n",
    "test_data  = pd.read_csv(data_dir+\"/test.csv\", parse_dates=[\"activation_date\"])\n",
    "train_data = train_data.replace(np.nan,-1,regex=True) #nan and other missing values are mapped to -1\n",
    "test_data  = test_data.replace(np.nan,-1,regex=True)\n",
    "\n",
    "##================Remove unwanted columns\n",
    "del train_data['image'], test_data['image'],train_data['user_id'],\n",
    "test_data['user_id'],train_data['item_id'],test_data['item_id']\n",
    "\n",
    "##================Replace Full Dates with Day-of-Week\n",
    "train_data['activation_date'] = train_data[\"activation_date\"].dt.weekday\n",
    "test_data['activation_date'] = test_data[\"activation_date\"].dt.weekday\n",
    "\n",
    "##================split into x_train/x_val. No stratification requried probably\n",
    "val_split = 0.15\n",
    "train_data = train_data.sample(frac=1).reset_index(drop=True)\n",
    "val_ix = int(np.rint(len(train_data)*(1.-val_split)))\n",
    "#data frame formats with y-values packed in\n",
    "train_df = train_data[:val_ix]\n",
    "val_df = train_data[val_ix:]\n",
    "test_df = test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dhiman\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:33: RuntimeWarning: divide by zero encountered in log1p\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "##================Create the Tokenizers\n",
    "region_tk = {x:i+1 for i, x in enumerate(train_df.region.unique())}#+1 because we want to reserve 0 for new but not missing values\n",
    "city_tk =  {x:i+1 for i, x in enumerate(train_df.city.unique())}\n",
    "cat1_tk =  {x:i+1 for i, x in enumerate(train_df.parent_category_name.unique())}\n",
    "cat2_tk =  {x:i+1 for i, x in enumerate(train_df.category_name.unique())}\n",
    "param1_tk =  {x:i+1 for i, x in enumerate(train_df.param_1.unique())}\n",
    "param2_tk =  {x:i+1 for i, x in enumerate(train_df.param_2.unique())}\n",
    "param3_tk =  {x:i+1 for i, x in enumerate(train_df.param_3.unique())}\n",
    "seqnum_tk =  {x:i+1 for i, x in enumerate(train_df.item_seq_number.unique())}\n",
    "usertype_tk = {x:i+1 for i, x in enumerate(train_df.user_type.unique())}\n",
    "imgtype_tk = {x:i+1 for i, x in enumerate(train_df.image_top_1.unique())}\n",
    "tokenizers = [region_tk, city_tk, cat1_tk, cat2_tk, param1_tk, param2_tk, param3_tk, seqnum_tk, usertype_tk, imgtype_tk]\n",
    "\n",
    "##================These functions are going to get repeated on train, val, and test data\n",
    "def tokenize_data(data, tokenizers):\n",
    "    region_tk, city_tk, cat1_tk, cat2_tk, param1_tk, param2_tk, param3_tk, seqnum_tk, usertype_tk, imgtype_tk = tokenizers\n",
    "    x_reg = np.asarray([region_tk.get(key, 0) for key in data.region], dtype=int)\n",
    "    x_city   = np.asarray([city_tk.get(key, 0) for key in data.city], dtype=int)\n",
    "    x_cat1   = np.asarray([cat1_tk.get(key, 0) for key in data.parent_category_name], dtype=int)\n",
    "    x_cat2   = np.asarray([cat2_tk.get(key, 0) for key in data.category_name], dtype=int)\n",
    "    x_prm1 = np.asarray([param1_tk.get(key, 0) for key in data.param_1], dtype=int)\n",
    "    x_prm2 = np.asarray([param2_tk.get(key, 0) for key in data.param_2], dtype=int)\n",
    "    x_prm3 = np.asarray([param3_tk.get(key, 0) for key in data.param_3], dtype=int)\n",
    "    x_sqnm = np.asarray([seqnum_tk.get(key, 0) for key in data.item_seq_number], dtype=int)\n",
    "    x_usr = np.asarray([usertype_tk.get(key, 0) for key in data.user_type], dtype=int)\n",
    "    x_itype = np.asarray([imgtype_tk.get(key, 0) for key in data.image_top_1], dtype=int)\n",
    "    return [x_reg, x_city, x_cat1, x_cat2, x_prm1, x_prm2, x_prm3, x_sqnm, x_usr, x_itype]\n",
    "\n",
    "def log_prices(data):\n",
    "    prices = data.price.as_matrix()\n",
    "    prices = np.log1p(prices)\n",
    "    prices[prices==-np.inf] = -1\n",
    "    return prices\n",
    "\n",
    "##================Final Processing on x, y train, val, test data\n",
    "x_train = tokenize_data(train_df, tokenizers)\n",
    "x_train.append(train_df.activation_date.as_matrix())\n",
    "x_train.append(log_prices(train_df))\n",
    "y_train = train_df.deal_probability.as_matrix()\n",
    "\n",
    "x_val = tokenize_data(val_df, tokenizers)\n",
    "x_val.append(val_df.activation_date.as_matrix())\n",
    "x_val.append(log_prices(val_df))\n",
    "y_val = val_df.deal_probability.as_matrix()\n",
    "\n",
    "x_test = tokenize_data(test_df, tokenizers)\n",
    "x_test.append(test_df.activation_date.as_matrix())\n",
    "x_test.append(log_prices(test_df))\n",
    "\n",
    "##================Beginning of the NN Model Outline. \n",
    "def build_model():\n",
    "    inp_reg = Input(shape=(1,))\n",
    "    inp_city = Input(shape=(1,))\n",
    "    inp_cat1 = Input(shape=(1,))\n",
    "    inp_cat2 = Input(shape=(1,))\n",
    "    inp_prm1 = Input(shape=(1,))\n",
    "    inp_prm2 = Input(shape=(1,))\n",
    "    inp_prm3 = Input(shape=(1,))\n",
    "    inp_sqnm = Input(shape=(1,))\n",
    "    inp_usr = Input(shape=(1,))\n",
    "    inp_itype = Input(shape=(1,))\n",
    "    inp_weekday = Input(shape=(1,))\n",
    "    inp_price = Input(shape=(1,))\n",
    "    nsy_price = GaussianNoise(0.1)(inp_price)\n",
    "    \n",
    "    emb_size = 32\n",
    "    emb_reg  = Embedding(len(region_tk)+1, emb_size)(inp_reg)\n",
    "    emb_city = Embedding(len(city_tk)+1, emb_size)(inp_city)\n",
    "    emb_cat1 = Embedding(len(cat1_tk)+1, emb_size)(inp_cat1)\n",
    "    emb_cat2 = Embedding(len(cat2_tk)+1, emb_size)(inp_cat2)\n",
    "    emb_prm1 = Embedding(len(param1_tk)+1, emb_size)(inp_prm1)\n",
    "    emb_prm2 = Embedding(len(param2_tk)+1, emb_size)(inp_prm2)\n",
    "    emb_prm3 = Embedding(len(param3_tk)+1, emb_size)(inp_prm3)\n",
    "    emb_sqnm = Embedding(len(seqnum_tk)+1, emb_size)(inp_sqnm)\n",
    "    emb_usr  = Embedding(len(usertype_tk)+1, emb_size)(inp_usr)\n",
    "    emb_itype= Embedding(len(imgtype_tk)+1, emb_size)(inp_itype)\n",
    "    x = concatenate([emb_reg,emb_city,emb_cat1,emb_cat2,emb_prm1,emb_prm2,emb_prm3,\n",
    "                     emb_sqnm,emb_usr,emb_itype])\n",
    "    x = Flatten()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = concatenate([x, nsy_price])#Do not want to dropout price, its noised up instead. \n",
    "    \n",
    "    x = Dense(512, activation=\"selu\", kernel_initializer=\"lecun_normal\")(x)\n",
    "    x = AlphaDropout(0.1)(x)\n",
    "    x = Dense(256, activation=\"selu\", kernel_initializer=\"lecun_normal\")(x)\n",
    "    x = AlphaDropout(0.1)(x)\n",
    "    x = Dense(128, activation=\"selu\", kernel_initializer=\"lecun_normal\")(x)\n",
    "    x = AlphaDropout(0.1)(x)\n",
    "    x = Dense(64, activation=\"selu\", kernel_initializer=\"lecun_normal\")(x)\n",
    "    x = AlphaDropout(0.05)(x)\n",
    "    x = Dense(32, activation=\"selu\", kernel_initializer=\"lecun_normal\")(x)\n",
    "    x = AlphaDropout(0.05)(x)\n",
    "    x = Dense(8, activation=\"selu\", kernel_initializer=\"lecun_normal\")(x)\n",
    "    y = Dense(1, activation=\"sigmoid\")(x)\n",
    "    \n",
    "    model = Model(inputs=[inp_reg, inp_city, inp_cat1, inp_cat2, inp_prm1, inp_prm2, \n",
    "                          inp_prm3, inp_sqnm, inp_usr, inp_itype, inp_weekday, inp_price],\n",
    "                  outputs=y)\n",
    "    model.compile(optimizer=\"Nadam\", loss=[\"MSE\"], metrics=[root_mean_squared_error])\n",
    "    model.summary()\n",
    "    \n",
    "    return model\n",
    "\n",
    "def root_mean_squared_error(y_true, y_pred):\n",
    "        return K.sqrt(K.mean(K.square(y_pred - y_true), axis=-1)) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_5 (InputLayer)            (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_6 (InputLayer)            (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_7 (InputLayer)            (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_8 (InputLayer)            (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_9 (InputLayer)            (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_10 (InputLayer)           (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 1, 32)        928         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 1, 32)        55264       input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, 1, 32)        320         input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_4 (Embedding)         (None, 1, 32)        1536        input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_5 (Embedding)         (None, 1, 32)        11904       input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_6 (Embedding)         (None, 1, 32)        8608        input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_7 (Embedding)         (None, 1, 32)        38112       input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_8 (Embedding)         (None, 1, 32)        821824      input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_9 (Embedding)         (None, 1, 32)        128         input_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_10 (Embedding)        (None, 1, 32)        98016       input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 1, 320)       0           embedding_1[0][0]                \n",
      "                                                                 embedding_2[0][0]                \n",
      "                                                                 embedding_3[0][0]                \n",
      "                                                                 embedding_4[0][0]                \n",
      "                                                                 embedding_5[0][0]                \n",
      "                                                                 embedding_6[0][0]                \n",
      "                                                                 embedding_7[0][0]                \n",
      "                                                                 embedding_8[0][0]                \n",
      "                                                                 embedding_9[0][0]                \n",
      "                                                                 embedding_10[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 320)          0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "input_12 (InputLayer)           (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 320)          0           flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "gaussian_noise_1 (GaussianNoise (None, 1)            0           input_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 321)          0           dropout_1[0][0]                  \n",
      "                                                                 gaussian_noise_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 512)          164864      concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "alpha_dropout_1 (AlphaDropout)  (None, 512)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 256)          131328      alpha_dropout_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "alpha_dropout_2 (AlphaDropout)  (None, 256)          0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 128)          32896       alpha_dropout_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "alpha_dropout_3 (AlphaDropout)  (None, 128)          0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 64)           8256        alpha_dropout_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "alpha_dropout_4 (AlphaDropout)  (None, 64)           0           dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 32)           2080        alpha_dropout_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "alpha_dropout_5 (AlphaDropout)  (None, 32)           0           dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 8)            264         alpha_dropout_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 1)            9           dense_6[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 1,376,337\n",
      "Trainable params: 1,376,337\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1277910 samples, validate on 225514 samples\n",
      "Epoch 1/100\n",
      " - 103s - loss: 0.0564 - root_mean_squared_error: 0.1602 - val_loss: 0.0543 - val_root_mean_squared_error: 0.1393\n",
      "Epoch 2/100\n",
      " - 8s - loss: 0.0534 - root_mean_squared_error: 0.1549 - val_loss: 0.0541 - val_root_mean_squared_error: 0.1366\n",
      "Epoch 3/100\n",
      " - 8s - loss: 0.0528 - root_mean_squared_error: 0.1535 - val_loss: 0.0549 - val_root_mean_squared_error: 0.1325\n",
      "Epoch 4/100\n",
      " - 8s - loss: 0.0524 - root_mean_squared_error: 0.1525 - val_loss: 0.0541 - val_root_mean_squared_error: 0.1357\n",
      "Epoch 5/100\n",
      " - 8s - loss: 0.0521 - root_mean_squared_error: 0.1519 - val_loss: 0.0534 - val_root_mean_squared_error: 0.1365\n",
      "Epoch 6/100\n",
      " - 8s - loss: 0.0518 - root_mean_squared_error: 0.1512 - val_loss: 0.0535 - val_root_mean_squared_error: 0.1338\n",
      "Epoch 7/100\n",
      " - 8s - loss: 0.0516 - root_mean_squared_error: 0.1507 - val_loss: 0.0548 - val_root_mean_squared_error: 0.1303\n",
      "Epoch 8/100\n",
      " - 8s - loss: 0.0514 - root_mean_squared_error: 0.1503 - val_loss: 0.0539 - val_root_mean_squared_error: 0.1326\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "Epoch 9/100\n",
      " - 8s - loss: 0.0508 - root_mean_squared_error: 0.1492 - val_loss: 0.0538 - val_root_mean_squared_error: 0.1327\n",
      "Epoch 10/100\n",
      " - 8s - loss: 0.0507 - root_mean_squared_error: 0.1490 - val_loss: 0.0536 - val_root_mean_squared_error: 0.1333\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 2.0000000949949027e-05.\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'input//sample_submission.csv' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-464af2764c66>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'model_baseline_weights.hdf5'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mpreds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0msubmission\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\"/sample_submission.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[0msubmission\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'deal_probability'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreds\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[0msubmission\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"submission.csv\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    707\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[0;32m    708\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 709\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    710\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    711\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    447\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    448\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 449\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    450\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    451\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    816\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    817\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 818\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    819\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    820\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1047\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'c'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1048\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'c'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1049\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1050\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1051\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'python'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1693\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'allow_leading_cols'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1694\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1695\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1696\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1697\u001b[0m         \u001b[1;31m# XXX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: File b'input//sample_submission.csv' does not exist"
     ]
    }
   ],
   "source": [
    "model = build_model()\n",
    "\n",
    "earlystop = EarlyStopping(monitor=\"val_loss\",mode=\"auto\",patience=5,verbose=0)\n",
    "checkpt = ModelCheckpoint(monitor=\"val_loss\",mode=\"auto\",filepath='model_baseline_weights.hdf5',verbose=0,save_best_only=True)\n",
    "rlrop = ReduceLROnPlateau(monitor='val_loss',mode='auto',patience=2,verbose=1,factor=0.1,cooldown=0,min_lr=1e-6)\n",
    "batch_size = 2048\n",
    "model.fit(x_train, y_train,batch_size=batch_size,validation_data=(x_val, y_val),\n",
    "          epochs=100,verbose=2,callbacks =[checkpt, earlystop, rlrop])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('model_baseline_weights.hdf5')\n",
    "preds = model.predict(x_test, batch_size=batch_size)\n",
    "submission = pd.read_csv(data_dir+\"/sample_submission.csv\")\n",
    "submission['deal_probability'] = preds\n",
    "submission.to_csv(\"submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

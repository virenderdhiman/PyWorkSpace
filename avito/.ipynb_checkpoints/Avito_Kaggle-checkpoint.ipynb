{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This model is designed to use embeddings to deal with the categorical variables, while ignoring \n",
    "some of the data like the periods, images, titles and descriptions. \n",
    "Most of the remaining data is categorical, except for the price. \n",
    "A simple FFNN model seems to do pretty good work. \n",
    "\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input, Embedding, Dropout, Flatten\n",
    "from keras.layers.merge import concatenate, dot, multiply, add\n",
    "from keras.callbacks import Callback, EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras.optimizers import Nadam, RMSprop, adam\n",
    "from keras.layers.noise import AlphaDropout, GaussianNoise\n",
    "from keras import backend as K\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##================Import Data, Replace NaNs with -1\n",
    "data_dir = \"../input/\"\n",
    "train_data = pd.read_csv(data_dir+\"/train.csv\", parse_dates=[\"activation_date\"]) #we will eventually turn the date column into day of week [0,6]\n",
    "test_data  = pd.read_csv(data_dir+\"/test.csv\", parse_dates=[\"activation_date\"])\n",
    "train_data = train_data.replace(np.nan,-1,regex=True) #nan and other missing values are mapped to -1\n",
    "test_data  = test_data.replace(np.nan,-1,regex=True)\n",
    "\n",
    "##================Remove unwanted columns\n",
    "del train_data['image'], test_data['image'],train_data['user_id'],\n",
    "test_data['user_id'],train_data['item_id'],test_data['item_id']\n",
    "\n",
    "##================Replace Full Dates with Day-of-Week\n",
    "train_data['activation_date'] = train_data[\"activation_date\"].dt.weekday\n",
    "test_data['activation_date'] = test_data[\"activation_date\"].dt.weekday\n",
    "\n",
    "##================split into x_train/x_val. No stratification requried probably\n",
    "val_split = 0.15\n",
    "train_data = train_data.sample(frac=1).reset_index(drop=True)\n",
    "val_ix = int(np.rint(len(train_data)*(1.-val_split)))\n",
    "#data frame formats with y-values packed in\n",
    "train_df = train_data[:val_ix]\n",
    "val_df = train_data[val_ix:]\n",
    "test_df = test_data\n",
    "\n",
    "##================Create the Tokenizers\n",
    "region_tk = {x:i+1 for i, x in enumerate(train_df.region.unique())}#+1 because we want to reserve 0 for new but not missing values\n",
    "city_tk =  {x:i+1 for i, x in enumerate(train_df.city.unique())}\n",
    "cat1_tk =  {x:i+1 for i, x in enumerate(train_df.parent_category_name.unique())}\n",
    "cat2_tk =  {x:i+1 for i, x in enumerate(train_df.category_name.unique())}\n",
    "param1_tk =  {x:i+1 for i, x in enumerate(train_df.param_1.unique())}\n",
    "param2_tk =  {x:i+1 for i, x in enumerate(train_df.param_2.unique())}\n",
    "param3_tk =  {x:i+1 for i, x in enumerate(train_df.param_3.unique())}\n",
    "seqnum_tk =  {x:i+1 for i, x in enumerate(train_df.item_seq_number.unique())}\n",
    "usertype_tk = {x:i+1 for i, x in enumerate(train_df.user_type.unique())}\n",
    "imgtype_tk = {x:i+1 for i, x in enumerate(train_df.image_top_1.unique())}\n",
    "tokenizers = [region_tk, city_tk, cat1_tk, cat2_tk, param1_tk, param2_tk, param3_tk, seqnum_tk, usertype_tk, imgtype_tk]\n",
    "\n",
    "##================These functions are going to get repeated on train, val, and test data\n",
    "def tokenize_data(data, tokenizers):\n",
    "    region_tk, city_tk, cat1_tk, cat2_tk, param1_tk, param2_tk, param3_tk, seqnum_tk, usertype_tk, imgtype_tk = tokenizers\n",
    "    x_reg = np.asarray([region_tk.get(key, 0) for key in data.region], dtype=int)\n",
    "    x_city   = np.asarray([city_tk.get(key, 0) for key in data.city], dtype=int)\n",
    "    x_cat1   = np.asarray([cat1_tk.get(key, 0) for key in data.parent_category_name], dtype=int)\n",
    "    x_cat2   = np.asarray([cat2_tk.get(key, 0) for key in data.category_name], dtype=int)\n",
    "    x_prm1 = np.asarray([param1_tk.get(key, 0) for key in data.param_1], dtype=int)\n",
    "    x_prm2 = np.asarray([param2_tk.get(key, 0) for key in data.param_2], dtype=int)\n",
    "    x_prm3 = np.asarray([param3_tk.get(key, 0) for key in data.param_3], dtype=int)\n",
    "    x_sqnm = np.asarray([seqnum_tk.get(key, 0) for key in data.item_seq_number], dtype=int)\n",
    "    x_usr = np.asarray([usertype_tk.get(key, 0) for key in data.user_type], dtype=int)\n",
    "    x_itype = np.asarray([imgtype_tk.get(key, 0) for key in data.image_top_1], dtype=int)\n",
    "    return [x_reg, x_city, x_cat1, x_cat2, x_prm1, x_prm2, x_prm3, x_sqnm, x_usr, x_itype]\n",
    "\n",
    "def log_prices(data):\n",
    "    prices = data.price.as_matrix()\n",
    "    prices = np.log1p(prices)\n",
    "    prices[prices==-np.inf] = -1\n",
    "    return prices\n",
    "\n",
    "##================Final Processing on x, y train, val, test data\n",
    "x_train = tokenize_data(train_df, tokenizers)\n",
    "x_train.append(train_df.activation_date.as_matrix())\n",
    "x_train.append(log_prices(train_df))\n",
    "y_train = train_df.deal_probability.as_matrix()\n",
    "\n",
    "x_val = tokenize_data(val_df, tokenizers)\n",
    "x_val.append(val_df.activation_date.as_matrix())\n",
    "x_val.append(log_prices(val_df))\n",
    "y_val = val_df.deal_probability.as_matrix()\n",
    "\n",
    "x_test = tokenize_data(test_df, tokenizers)\n",
    "x_test.append(test_df.activation_date.as_matrix())\n",
    "x_test.append(log_prices(test_df))\n",
    "\n",
    "##================Beginning of the NN Model Outline. \n",
    "def build_model():\n",
    "    inp_reg = Input(shape=(1,))\n",
    "    inp_city = Input(shape=(1,))\n",
    "    inp_cat1 = Input(shape=(1,))\n",
    "    inp_cat2 = Input(shape=(1,))\n",
    "    inp_prm1 = Input(shape=(1,))\n",
    "    inp_prm2 = Input(shape=(1,))\n",
    "    inp_prm3 = Input(shape=(1,))\n",
    "    inp_sqnm = Input(shape=(1,))\n",
    "    inp_usr = Input(shape=(1,))\n",
    "    inp_itype = Input(shape=(1,))\n",
    "    inp_weekday = Input(shape=(1,))\n",
    "    inp_price = Input(shape=(1,))\n",
    "    nsy_price = GaussianNoise(0.1)(inp_price)\n",
    "    \n",
    "    emb_size = 32\n",
    "    emb_reg  = Embedding(len(region_tk)+1, emb_size)(inp_reg)\n",
    "    emb_city = Embedding(len(city_tk)+1, emb_size)(inp_city)\n",
    "    emb_cat1 = Embedding(len(cat1_tk)+1, emb_size)(inp_cat1)\n",
    "    emb_cat2 = Embedding(len(cat2_tk)+1, emb_size)(inp_cat2)\n",
    "    emb_prm1 = Embedding(len(param1_tk)+1, emb_size)(inp_prm1)\n",
    "    emb_prm2 = Embedding(len(param2_tk)+1, emb_size)(inp_prm2)\n",
    "    emb_prm3 = Embedding(len(param3_tk)+1, emb_size)(inp_prm3)\n",
    "    emb_sqnm = Embedding(len(seqnum_tk)+1, emb_size)(inp_sqnm)\n",
    "    emb_usr  = Embedding(len(usertype_tk)+1, emb_size)(inp_usr)\n",
    "    emb_itype= Embedding(len(imgtype_tk)+1, emb_size)(inp_itype)\n",
    "    x = concatenate([emb_reg,emb_city,emb_cat1,emb_cat2,emb_prm1,emb_prm2,emb_prm3,\n",
    "                     emb_sqnm,emb_usr,emb_itype])\n",
    "    x = Flatten()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = concatenate([x, nsy_price])#Do not want to dropout price, its noised up instead. \n",
    "    \n",
    "    x = Dense(512, activation=\"selu\", kernel_initializer=\"lecun_normal\")(x)\n",
    "    x = AlphaDropout(0.1)(x)\n",
    "    x = Dense(256, activation=\"selu\", kernel_initializer=\"lecun_normal\")(x)\n",
    "    x = AlphaDropout(0.1)(x)\n",
    "    x = Dense(128, activation=\"selu\", kernel_initializer=\"lecun_normal\")(x)\n",
    "    x = AlphaDropout(0.1)(x)\n",
    "    x = Dense(64, activation=\"selu\", kernel_initializer=\"lecun_normal\")(x)\n",
    "    x = AlphaDropout(0.05)(x)\n",
    "    x = Dense(32, activation=\"selu\", kernel_initializer=\"lecun_normal\")(x)\n",
    "    x = AlphaDropout(0.05)(x)\n",
    "    x = Dense(8, activation=\"selu\", kernel_initializer=\"lecun_normal\")(x)\n",
    "    y = Dense(1, activation=\"sigmoid\")(x)\n",
    "    \n",
    "    model = Model(inputs=[inp_reg, inp_city, inp_cat1, inp_cat2, inp_prm1, inp_prm2, \n",
    "                          inp_prm3, inp_sqnm, inp_usr, inp_itype, inp_weekday, inp_price],\n",
    "                  outputs=y)\n",
    "    model.compile(optimizer=\"Nadam\", loss=[\"MSE\"], metrics=[root_mean_squared_error])\n",
    "    model.summary()\n",
    "    \n",
    "    return model\n",
    "\n",
    "def root_mean_squared_error(y_true, y_pred):\n",
    "        return K.sqrt(K.mean(K.square(y_pred - y_true), axis=-1)) \n",
    "\n",
    "model = build_model()\n",
    "\n",
    "earlystop = EarlyStopping(monitor=\"val_loss\",mode=\"auto\",patience=5,verbose=0)\n",
    "checkpt = ModelCheckpoint(monitor=\"val_loss\",mode=\"auto\",filepath='model_baseline_weights.hdf5',verbose=0,save_best_only=True)\n",
    "rlrop = ReduceLROnPlateau(monitor='val_loss',mode='auto',patience=2,verbose=1,factor=0.1,cooldown=0,min_lr=1e-6)\n",
    "batch_size = 2048\n",
    "model.fit(x_train, y_train,batch_size=batch_size,validation_data=(x_val, y_val),\n",
    "          epochs=100,verbose=2,callbacks =[checkpt, earlystop, rlrop])\n",
    "\n",
    "model.load_weights('model_baseline_weights.hdf5')\n",
    "preds = model.predict(x_test, batch_size=batch_size)\n",
    "submission = pd.read_csv(data_dir+\"/sample_submission.csv\")\n",
    "submission['deal_probability'] = preds\n",
    "submission.to_csv(\"submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
